"use strict";(self.webpackChunkkoordinator_sh=self.webpackChunkkoordinator_sh||[]).push([[602],{3905:function(e,o,t){t.d(o,{Zo:function(){return c},kt:function(){return h}});var n=t(7294);function i(e,o,t){return o in e?Object.defineProperty(e,o,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[o]=t,e}function a(e,o){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);o&&(n=n.filter((function(o){return Object.getOwnPropertyDescriptor(e,o).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var o=1;o<arguments.length;o++){var t=null!=arguments[o]?arguments[o]:{};o%2?a(Object(t),!0).forEach((function(o){i(e,o,t[o])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach((function(o){Object.defineProperty(e,o,Object.getOwnPropertyDescriptor(t,o))}))}return e}function r(e,o){if(null==e)return{};var t,n,i=function(e,o){if(null==e)return{};var t,n,i={},a=Object.keys(e);for(n=0;n<a.length;n++)t=a[n],o.indexOf(t)>=0||(i[t]=e[t]);return i}(e,o);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)t=a[n],o.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=n.createContext({}),d=function(e){var o=n.useContext(s),t=o;return e&&(t="function"==typeof e?e(o):l(l({},o),e)),t},c=function(e){var o=d(e.components);return n.createElement(s.Provider,{value:o},e.children)},p={inlineCode:"code",wrapper:function(e){var o=e.children;return n.createElement(n.Fragment,{},o)}},u=n.forwardRef((function(e,o){var t=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,c=r(e,["components","mdxType","originalType","parentName"]),u=d(t),h=i,m=u["".concat(s,".").concat(h)]||u[h]||p[h]||a;return t?n.createElement(m,l(l({ref:o},c),{},{components:t})):n.createElement(m,l({ref:o},c))}));function h(e,o){var t=arguments,i=o&&o.mdxType;if("string"==typeof e||i){var a=t.length,l=new Array(a);l[0]=u;var r={};for(var s in o)hasOwnProperty.call(o,s)&&(r[s]=o[s]);r.originalType=e,r.mdxType="string"==typeof e?e:i,l[1]=r;for(var d=2;d<a;d++)l[d]=t[d];return n.createElement.apply(null,l)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},2181:function(e,o,t){t.r(o),t.d(o,{assets:function(){return c},contentTitle:function(){return s},default:function(){return h},frontMatter:function(){return r},metadata:function(){return d},toc:function(){return p}});var n=t(7462),i=t(3366),a=(t(7294),t(3905)),l=["components"],r={},s="Fine-grained CPU orchestration",d={unversionedId:"designs/fine-grained-cpu-orchestration",id:"version-v0.5/designs/fine-grained-cpu-orchestration",title:"Fine-grained CPU orchestration",description:"Summary",source:"@site/versioned_docs/version-v0.5/designs/fine-grained-cpu-orchestration.md",sourceDirName:"designs",slug:"/designs/fine-grained-cpu-orchestration",permalink:"/zh-Hans/docs/v0.5/designs/fine-grained-cpu-orchestration",editUrl:"https://github.com/koordinator-sh/koordinator.sh/edit/main/docs/designs/fine-grained-cpu-orchestration.md",tags:[],version:"v0.5",lastUpdatedBy:"Jason Liu",lastUpdatedAt:1656909557,formattedLastUpdatedAt:"2022/7/4",frontMatter:{},sidebar:"docs",previous:{title:"Load-aware Scheduling",permalink:"/zh-Hans/docs/v0.5/designs/load-aware-scheduling"},next:{title:"Colocation of Spark Jobs",permalink:"/zh-Hans/docs/v0.5/best-practices/colocation-of-spark-jobs"}},c={},p=[{value:"Summary",id:"summary",level:2},{value:"Motivation",id:"motivation",level:2},{value:"Goals",id:"goals",level:3},{value:"Non-Goals/Future Work",id:"non-goalsfuture-work",level:3},{value:"Design Overview",id:"design-overview",level:2},{value:"User stories",id:"user-stories",level:2},{value:"Story 1",id:"story-1",level:3},{value:"Story 2",id:"story-2",level:3},{value:"Story 3",id:"story-3",level:3},{value:"Story 4",id:"story-4",level:3},{value:"Story 5",id:"story-5",level:3},{value:"Story 6",id:"story-6",level:3},{value:"CPU orchestration principles",id:"cpu-orchestration-principles",level:2},{value:"Koordinator QoS CPU orchestration principles",id:"koordinator-qos-cpu-orchestration-principles",level:3},{value:"Compatible kubelet CPU management policies",id:"compatible-kubelet-cpu-management-policies",level:3},{value:"Take over kubelet CPU management policies",id:"take-over-kubelet-cpu-management-policies",level:3},{value:"CPU orchestration API",id:"cpu-orchestration-api",level:2},{value:"Application CPU orchestration API",id:"application-cpu-orchestration-api",level:3},{value:"Resource spec",id:"resource-spec",level:4},{value:"Resource status",id:"resource-status",level:4},{value:"Example",id:"example",level:4},{value:"Node CPU orchestration API",id:"node-cpu-orchestration-api",level:3},{value:"CPU bind policy",id:"cpu-bind-policy",level:4},{value:"NUMA allocate strategy",id:"numa-allocate-strategy",level:4},{value:"NUMA topology alignment policy",id:"numa-topology-alignment-policy",level:4},{value:"Example",id:"example-1",level:4},{value:"NodeResourceTopology CRD",id:"noderesourcetopology-crd",level:3},{value:"CRD Scheme definition",id:"crd-scheme-definition",level:4},{value:"Compatible",id:"compatible",level:4},{value:"Extension",id:"extension",level:4},{value:"Create/Update NodeResourceTopology",id:"createupdate-noderesourcetopology",level:4},{value:"Example",id:"example-2",level:4}],u={toc:p};function h(e){var o=e.components,r=(0,i.Z)(e,l);return(0,a.kt)("wrapper",(0,n.Z)({},u,r,{components:o,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"fine-grained-cpu-orchestration"},"Fine-grained CPU orchestration"),(0,a.kt)("h2",{id:"summary"},"Summary"),(0,a.kt)("p",null,"This proposal defines the fine-grained CPU orchestration for Koordinator QoS in detail, and how to be compatible with the existing design principles and implementations of K8s. This proposal describes the functionality that koordlet, koord-runtime-proxy and koord-scheduler need to enhance."),(0,a.kt)("h2",{id:"motivation"},"Motivation"),(0,a.kt)("p",null,"An increasing number of systems leverage a combination of CPUs and hardware accelerators to support latency-critical execution and high-throughput parallel computation. These include workloads in fields such as telecommunications, scientific computing, machine learning, financial services and data analytics. Such hybrid systems comprise a high performance environment."),(0,a.kt)("p",null,"In order to extract the best performance, optimizations related to CPU isolation, NUMA-locality are required."),(0,a.kt)("h3",{id:"goals"},"Goals"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Improve the CPU orchestration semantics of Koordinator QoS."),(0,a.kt)("li",{parentName:"ol"},"Determine compatible kubelet policies."),(0,a.kt)("li",{parentName:"ol"},"Clarify how koordlet should enhance CPU scheduling mechanism."),(0,a.kt)("li",{parentName:"ol"},"Provide a set of API such as CPU bind policies, CPU exclusive policies, NUMA topology alignment policies, NUMA topology information, etc. for applications and cluster administrator to support complex CPU orchestration scenarios."),(0,a.kt)("li",{parentName:"ol"},"Provide the CPU orchestration optimization API.")),(0,a.kt)("h3",{id:"non-goalsfuture-work"},"Non-Goals/Future Work"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Describe specific design details of koordlet/koord-runtime-proxy."),(0,a.kt)("li",{parentName:"ol"},"Describe specific design details of CPU descheduling mechanism.")),(0,a.kt)("h2",{id:"design-overview"},"Design Overview"),(0,a.kt)("p",null,(0,a.kt)("img",{loading:"lazy",alt:"image",src:t(9252).Z,width:"1251",height:"901"})),(0,a.kt)("p",null,"When koordlet starts, koordlet gather the NUMA topology information from kubelet include NUMA Topology, CPU Topology, kubelet cpu management policy, kubelet allocated CPUs for Guaranteed Pods etc., and update to the NodeResourceTopology CRD. The latency-sensitive applications are scaling, the new Pod can set Koordinator QoS with LSE/LSR, CPU Bind policy and CPU exclusive policy to require koord-scheduler to allocate best-fit CPUs to get the best performance. When koord-scheduler scheduling the Pod, koord-scheduler will filter Nodes that satisfied NUMA Topology Alignment policy, and select the best Node by scoring, allocating the CPUs in Reserve phase, and records the result to Pod annotation when PreBinding. koordlet hooks the kubelet CRI request to replace the CPUs configuration parameters with the koord-scheduler scheduled result to the runtime such as configure the cgroup."),(0,a.kt)("h2",{id:"user-stories"},"User stories"),(0,a.kt)("h3",{id:"story-1"},"Story 1"),(0,a.kt)("p",null,"Compatible with kubelet's existing CPU management policies. The CPU manager policy ",(0,a.kt)("inlineCode",{parentName:"p"},"static")," allows pods with certain resource characteristics to be granted increased CPU affinity and exclusivity in the node. If enabled the ",(0,a.kt)("inlineCode",{parentName:"p"},"static")," policy, the cluster administrator must configure the kubelet reserve some CPUs. There are some options for ",(0,a.kt)("inlineCode",{parentName:"p"},"static")," policy. If the ",(0,a.kt)("inlineCode",{parentName:"p"},"full-pcpus-only(beta, visible by default)")," policy option is specified, the ",(0,a.kt)("inlineCode",{parentName:"p"},"static")," policy will always allocate full physical cores. If the ",(0,a.kt)("inlineCode",{parentName:"p"},"distribute-cpus-across-numa(alpha, hidden by default)")," option is specified, the ",(0,a.kt)("inlineCode",{parentName:"p"},"static")," policy will evenly distribute CPUs across NUMA nodes in cases where more than one NUMA node is required to satisfy the allocation."),(0,a.kt)("h3",{id:"story-2"},"Story 2"),(0,a.kt)("p",null,"Similarly, the semantics of the existing K8s Guaranteed Pods in the community should be compatible. The cpu cores allocated to K8s Guaranteed Pods with ",(0,a.kt)("inlineCode",{parentName:"p"},"static")," policy will not share to the default best effort Pods, so it is equivalent to LSE. But when the load in the node is relatively low, the CPUs allocated by LSR Pods should be shared with best effort workloads to obtain economic benefits. "),(0,a.kt)("h3",{id:"story-3"},"Story 3"),(0,a.kt)("p",null,"The Topology Manager is a kubelet component that aims to coordinate the set of components that are responsible for these optimizations. After Topology Manager was introduced the problem of launching pod in the cluster where worker nodes have different NUMA topology and different amount of resources in that topology became actual. The Pod could be scheduled in the node where the total amount of resources is enough, but resource distribution could not satisfy the appropriate Topology policy. "),(0,a.kt)("h3",{id:"story-4"},"Story 4"),(0,a.kt)("p",null,"The scheduler can coordinate the arrangement between latency-sensitive applications. For example, the same latency-sensitive applications can be mutually exclusive in the CPU dimension, and latency-sensitive applications and general applications can be deployed in the CPU dimension affinity. Costs can be reduced and runtime quality can be guaranteed."),(0,a.kt)("h3",{id:"story-5"},"Story 5"),(0,a.kt)("p",null,"When allocating CPUs based on NUMA topology, users want to have different allocation strategies. For example, bin-packing takes precedence, or assigns the most idle NUMA Node."),(0,a.kt)("h3",{id:"story-6"},"Story 6"),(0,a.kt)("p",null,"As the application scaling or rolling deployment, the best-fit allocatable space will gradually become fragmented, which will lead to the bad allocation effect of some strategies and affect the runtime effect of the application. "),(0,a.kt)("h2",{id:"cpu-orchestration-principles"},"CPU orchestration principles"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Only supports the CPU allocation mechanism of the Pod dimension."),(0,a.kt)("li",{parentName:"ol"},"Koordinator divides the CPU on the machine into ",(0,a.kt)("inlineCode",{parentName:"li"},"CPU Shared Pool"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"statically exclusive CPUs")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"BE CPU Shared Pool"),". ",(0,a.kt)("ol",{parentName:"li"},(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"CPU Shared Pool")," is the set of CPUs on which any containers in ",(0,a.kt)("inlineCode",{parentName:"li"},"K8s Burstable")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"Koordinator LSE")," Pods run. Containers in ",(0,a.kt)("inlineCode",{parentName:"li"},"K8s Guaranteed")," pods with ",(0,a.kt)("inlineCode",{parentName:"li"},"fractional CPU requests")," also run on CPUs in the shared pool. The shared pool contains all unallocated CPUs in the node but excluding CPUs allocated by K8s Guaranteed, LSE and LSR Pods. If kubelet reserved CPUs, the shared pool includes the reserved CPUs. "),(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"statically exclusive CPUs")," are the set of CPUs on which any containers in ",(0,a.kt)("inlineCode",{parentName:"li"},"K8s Guaranteed"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"Koordinator LSE")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"LSR")," Pods that have integer CPU run. When K8s Guaranteed, LSE and LSR Pods request CPU, koord-scheduler will be allocated from the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPU Shared Pool"),"."),(0,a.kt)("li",{parentName:"ol"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"BE CPU Shared pool")," is the set of CPUs on which any containers in ",(0,a.kt)("inlineCode",{parentName:"li"},"K8s BestEffort")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"Koordinator BE")," Pods run. The ",(0,a.kt)("inlineCode",{parentName:"li"},"BE CPU Shared Pool")," contains all CPUs in the node but excluding CPUs allocated by K8s Guaranteed and Koordinator Pods.")))),(0,a.kt)("h3",{id:"koordinator-qos-cpu-orchestration-principles"},"Koordinator QoS CPU orchestration principles"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"The Request and Limit of LSE/LSR Pods ",(0,a.kt)("strong",{parentName:"li"},"MUST BE")," equal and the CPU value ",(0,a.kt)("strong",{parentName:"li"},"MUST BE")," an integer multiple of 1000."),(0,a.kt)("li",{parentName:"ol"},"The CPUs allocated by the LSE Pod are completely ",(0,a.kt)("strong",{parentName:"li"},"exclusive")," and ",(0,a.kt)("strong",{parentName:"li"},"MUST NOT BE")," shared. If the node is hyper-threading architecture, only the logical core dimension is guaranteed to be isolated, but better isolation can be obtained through the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPUBindPolicyFullPCPUs")," policy."),(0,a.kt)("li",{parentName:"ol"},"The CPUs allocated by the LSR Pod only can be shared with BE Pods."),(0,a.kt)("li",{parentName:"ol"},"LS Pods bind the CPU shared pool that is ",(0,a.kt)("strong",{parentName:"li"},"exclusive")," with LSE/LSR Pods."),(0,a.kt)("li",{parentName:"ol"},"BE Pods bind all CPUs in the node ",(0,a.kt)("strong",{parentName:"li"},"exclusive")," with LSE Pods."),(0,a.kt)("li",{parentName:"ol"},"The K8s Guaranteed Pods already running is equivalent to Koordinator LSR if kubelet enables the CPU manager ",(0,a.kt)("inlineCode",{parentName:"li"},"static")," policy."),(0,a.kt)("li",{parentName:"ol"},"The K8s Guaranteed Pods already running is equivalent to Koordinator LS if kubelet enables the CPU manager ",(0,a.kt)("inlineCode",{parentName:"li"},"none")," policy."),(0,a.kt)("li",{parentName:"ol"},"Newly created K8s Guaranteed Pod without Koordinator QoS specified will be treated as LS.")),(0,a.kt)("h3",{id:"compatible-kubelet-cpu-management-policies"},"Compatible kubelet CPU management policies"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"If kubelet set the CPU manager policy options ",(0,a.kt)("inlineCode",{parentName:"li"},"full-pcpus-only=true")," / ",(0,a.kt)("inlineCode",{parentName:"li"},"distribute-cpus-across-numa=true"),", and there is no new CPU bind policy defined by Koordinator in the node, follow the semantics of these parameters defined by the kubelet."),(0,a.kt)("li",{parentName:"ol"},"If kubelet set the Topology manager policy, and there is no new NUMA Topology Alignment policy defined by Koordinator in the node, follow the semantics of these parameters defined by the kubelet. ")),(0,a.kt)("h3",{id:"take-over-kubelet-cpu-management-policies"},"Take over kubelet CPU management policies"),(0,a.kt)("p",null,"Because the CPU reserved by kubelet mainly serves K8s BestEffort and Burstable Pods. But Koordinator will not follow the policy. The K8s Burstable Pods should use the CPU Shared Pool, and the K8s BestEffort Pods should use the ",(0,a.kt)("inlineCode",{parentName:"p"},"BE CPU Shared Pool"),". The Koordinator LSE and LSR Pods can allocate from the CPU reserved by the kubelet."),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"For K8s Burstable and Koordinator LS Pods:",(0,a.kt)("ol",{parentName:"li"},(0,a.kt)("li",{parentName:"ol"},"When the koordlet starts, calculates the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPU Shared Pool")," and applies the shared pool to all Burstable and LS Pods in the node, that is, updating their cgroups to set cpuset. The same logic is executed when LSE/LSR Pods are creating or destroying. "),(0,a.kt)("li",{parentName:"ol"},"koordlet ignore the CPUs reserved by kubelet, and replace them with CPU Shared Pool defined by Koordinator. "))),(0,a.kt)("li",{parentName:"ol"},"For K8s BestEffort and Koordinator BE Pods:",(0,a.kt)("ol",{parentName:"li"},(0,a.kt)("li",{parentName:"ol"},"If kubelet reserved CPUs, the best effort Pods use the reserved CPUs first."),(0,a.kt)("li",{parentName:"ol"},"koordlet can use all CPUs in the node but exclude the CPUs allocated by K8s Guaranteed and Koordinator LSE Pods that have integer CPU. It means that if koordlet enables the CPU Suppress feature should follow the constraint to guarantee not affecting LSE Pods. Similarly, if kubelet enables the CPU manager policy with ",(0,a.kt)("inlineCode",{parentName:"li"},"static"),", the K8s Guaranteed Pods should also be excluded. "))),(0,a.kt)("li",{parentName:"ol"},"For K8s Guaranteed Pods:",(0,a.kt)("ol",{parentName:"li"},(0,a.kt)("li",{parentName:"ol"},"If there is ",(0,a.kt)("inlineCode",{parentName:"li"},"scheduling.koordinator.sh/resource-status")," updated by koord-scheduler in the Pod annotation, then replace the CPUSet in the kubelet CRI request, including Sandbox/Container creating stage."),(0,a.kt)("li",{parentName:"ol"},"kubelet sometimes call ",(0,a.kt)("inlineCode",{parentName:"li"},"Update")," method defined in CRI to update container cgroup to set new CPUs, so koordlet and koord-runtime-proxy need to hook the method."))),(0,a.kt)("li",{parentName:"ol"},"Automatically resize CPU Shared Pool",(0,a.kt)("ol",{parentName:"li"},(0,a.kt)("li",{parentName:"ol"},"koordlet automatically resize ",(0,a.kt)("inlineCode",{parentName:"li"},"CPU Shared Pool")," based on the changes such as Pod creating/destroying. If ",(0,a.kt)("inlineCode",{parentName:"li"},"CPU Shared Pool")," changed, koordlet should update cgroups of all LS/K8s Burstable Pods with the CPUs of shared pool. "),(0,a.kt)("li",{parentName:"ol"},"If the corresponding ",(0,a.kt)("inlineCode",{parentName:"li"},"CPU Shared Pool")," is specified in the annotation ",(0,a.kt)("inlineCode",{parentName:"li"},"scheduling.koordinator.sh/resource-status")," of the Pod, koordlet need to bind only the CPUs of the corresponding pool when configuring the cgroup.")))),(0,a.kt)("p",null,"The takeover logic will require koord-runtime-proxy to add new extension points, and require koordlet to implement a new runtime hook plugin. When koord-runtime-proxy is not installed, these takeover logic will also be able to be implemented."),(0,a.kt)("h2",{id:"cpu-orchestration-api"},"CPU orchestration API"),(0,a.kt)("h3",{id:"application-cpu-orchestration-api"},"Application CPU orchestration API"),(0,a.kt)("h4",{id:"resource-spec"},"Resource spec"),(0,a.kt)("p",null,"The annotation ",(0,a.kt)("inlineCode",{parentName:"p"},"scheduling.koordinator.sh/resource-spec")," is a resource allocation API defined by Koordinator. The user specifies the desired CPU orchestration policy by setting the annotation. In the future, we can also extend and add resource types that need to be supported as needed. The scheme corresponding to the annotation value is defined as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'// ResourceSpec describes extra attributes of the compute resource requirements.\ntype ResourceSpec struct {\n  PreferredCPUBindPolicy       CPUBindPolicy      `json:"preferredCPUBindPolicy,omitempty"`\n  PreferredCPUExclusivePolicy  CPUExclusivePolicy `json:"preferredCPUExclusivePolicy,omitempty"`\n}\n\ntype CPUBindPolicy string\n\nconst (\n  // CPUBindPolicyNone does not perform any bind policy\n  CPUBindPolicyNone CPUBindPolicy = "None"\n  // CPUBindPolicyFullPCPUs favor cpuset allocation that pack in few physical cores\n  CPUBindPolicyFullPCPUs CPUBindPolicy = "FullPCPUs"\n  // CPUBindPolicySpreadByPCPUs favor cpuset allocation that evenly allocate logical cpus across physical cores\n  CPUBindPolicySpreadByPCPUs CPUBindPolicy = "SpreadByPCPUs"\n  // CPUBindPolicyConstrainedBurst constrains the CPU Shared Pool range of the Burstable Pod\n  CPUBindPolicyConstrainedBurst CPUBindPolicy = "ConstrainedBurst"\n)\n\ntype CPUExclusivePolicy string\n\nconst (\n  // CPUExclusivePolicyNone does not perform any exclusive policy\n  CPUExclusivePolicyNone CPUExclusivePolicy = "None"\n  // CPUExclusivePolicyPCPULevel represents mutual exclusion in the physical core dimension \n  CPUExclusivePolicyPCPULevel CPUExclusivePolicy = "PCPULevel"\n  // CPUExclusivePolicyNUMANodeLevel indicates mutual exclusion in the NUMA topology dimension\n  CPUExclusivePolicyNUMANodeLevel CPUExclusivePolicy = "NUMANodeLevel"\n)\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"CPUBindPolicy")," defines the CPU binding policy. The specific values are defined as follows:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"CPUBindPolicyNone")," or empty value does not perform any bind policy. It is completely determined by the scheduler plugin configuration."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"CPUBindPolicyFullPCPUs")," is a bin-packing policy, similar to the ",(0,a.kt)("inlineCode",{parentName:"li"},"full-pcpus-only=true")," option defined by the kubelet, that allocate full physical cores. However, if the number of remaining logical CPUs in the node is sufficient but the number of full physical cores is insufficient, the allocation will continue. This policy can effectively avoid the noisy neighbor problem."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"CPUBindPolicySpreadByPCPUs")," is a spread policy. If the node enabled Hyper-Threading, when this policy is adopted, the scheduler will evenly allocate logical CPUs across physical cores. For example, the current node has 8 physical cores and 16 logical CPUs. When a Pod requires 8 logical CPUs and the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPUBindPolicySpreadByPCPUs")," policy is adopted, the scheduler will allocate an logical CPU from each physical core. This policy is mainly used by some latency-sensitive applications with multiple different peak-to-valley characteristics. It can not only allow the application to fully use the CPU at certain times, but will not be disturbed by the application on the same physical core. So the noisy neighbor problem may arise when using this policy."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"CPUBindPolicyConstrainedBurst")," a special policy that mainly helps K8s Burstable/Koordinator LS Pod get better performance. When using the policy, koord-scheduler is filtering out Nodes that have NUMA Nodes with suitable CPU Shared Pool by Pod Limit. After the scheduling is successful, the scheduler will update ",(0,a.kt)("inlineCode",{parentName:"li"},"scheduling.koordinator.sh/resource-status")," in the Pod, declaring the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPU Shared Pool")," to be bound. The koordlet binds the CPU Shared Pool of the corresponding NUMA Node according to the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPU Shared Pool")),(0,a.kt)("li",{parentName:"ul"},"If ",(0,a.kt)("inlineCode",{parentName:"li"},"kubelet.koordinator.sh/cpu-manager-policy")," in ",(0,a.kt)("inlineCode",{parentName:"li"},"NodeResourceTopology")," has option ",(0,a.kt)("inlineCode",{parentName:"li"},"full-pcpus-only=true"),", or ",(0,a.kt)("inlineCode",{parentName:"li"},"node.koordinator.sh/cpu-bind-policy")," in the Node with the value ",(0,a.kt)("inlineCode",{parentName:"li"},"PCPUOnly"),", the koord-scheduler will check whether the number of CPU requests of the Pod meets the ",(0,a.kt)("inlineCode",{parentName:"li"},"SMT-alignment")," requirements, so as to avoid being rejected by the kubelet after scheduling. koord-scheduler will avoid such nodes if the Pod uses the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPUBindPolicySpreadByPCPUs")," policy or the number of logical CPUs mapped to the number of physical cores is not an integer."))),(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"CPUExclusivePolicy")," defines the CPU exclusive policy, it can help users to avoid noisy neighbor problems. The specific values are defined as follows:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"CPUExclusivePolicyNone")," or empty value does not perform any isolate policy. It is completely determined by the scheduler plugin configuration."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"CPUExclusivePolicyPCPULevel"),". When allocating logical CPUs, try to avoid physical cores that have already been applied for by the same exclusive policy. It is a supplement to the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPUBindPolicySpreadByPCPUs")," policy. "),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"CPUExclusivePolicyNUMANodeLevel"),". When allocating logical CPUs, try to avoid NUMA Nodes that has already been applied for by the same exclusive policy. If there is no NUMA Node that satisfies the policy, downgrade to ",(0,a.kt)("inlineCode",{parentName:"li"},"PCPU")," policy.")))),(0,a.kt)("p",null,"For the ARM architecture, ",(0,a.kt)("inlineCode",{parentName:"p"},"CPUBindPolicy")," only support ",(0,a.kt)("inlineCode",{parentName:"p"},"CPUBindPolicyFullPCPUs"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"CPUExclusivePolicy")," only support ",(0,a.kt)("inlineCode",{parentName:"p"},"CPUExclusivePolicyNUMANodeLevel"),"."),(0,a.kt)("h4",{id:"resource-status"},"Resource status"),(0,a.kt)("p",null,"The annotation ",(0,a.kt)("inlineCode",{parentName:"p"},"scheduling.koordinator.sh/resource-status")," represents resource allocation result. koord-scheduler patch Pod with the annotation before binding to node. koordlet uses the result to configure cgroup."),(0,a.kt)("p",null,"The scheme corresponding to the annotation value is defined as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'type ResourceStatus struct {\n  CPUSet         string          `json:"cpuset,omitempty"`\n  CPUSharedPools []CPUSharedPool `json:"cpuSharedPools,omitempty"`\n}\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"CPUSet")," represents the allocated CPUs. When LSE/LSR Pod requested, koord-scheduler will update the field. It is Linux CPU list formatted string. For more details, please refer to ",(0,a.kt)("a",{parentName:"li",href:"http://man7.org/linux/man-pages/man7/cpuset.7.html#FORMATS"},"doc"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"CPUSharedPools")," represents the desired CPU Shared Pools used by LS Pods. If the Node has the label ",(0,a.kt)("inlineCode",{parentName:"li"},"node.koordinator.sh/numa-topology-alignment-policy")," with ",(0,a.kt)("inlineCode",{parentName:"li"},"Restricted/SingleNUMANode"),", koord-scheduler will find the best-fit NUMA Node for the LS Pod, and update the field that requires koordlet uses the specified CPU Shared Pool. It should be noted that the scheduler does not update the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPUSet")," field in the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPUSharedPool"),", koordlet binds the CPU Shared Pool of the corresponding NUMA Node according to the ",(0,a.kt)("inlineCode",{parentName:"li"},"SocketID")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"NodeID")," fields in the ",(0,a.kt)("inlineCode",{parentName:"li"},"CPUSharedPool"),".")),(0,a.kt)("h4",{id:"example"},"Example"),(0,a.kt)("p",null,"The following specific example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduling.koordinator.sh/resource-spec: |-\n      {\n        "preferredCPUBindPolicy": "SpreadByPCPUs",\n        "preferredCPUExclusivePolicy": "PCPULevel"\n      }\n    scheduling.koordinator.sh/resource-status: |-\n      {\n        "cpuset": "0-3"\n      }\n  name: test-pod\n  namespace: default\nspec:\n  ...\n')),(0,a.kt)("h3",{id:"node-cpu-orchestration-api"},"Node CPU orchestration API"),(0,a.kt)("p",null,"From the perspective of cluster administrators, it is necessary to support some APIs to control the CPU orchestration behavior of nodes."),(0,a.kt)("h4",{id:"cpu-bind-policy"},"CPU bind policy"),(0,a.kt)("p",null,"The label ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/cpu-bind-policy")," constrains how to bind CPU logical CPUs when scheduling. "),(0,a.kt)("p",null,"The following is the specific value definition:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"None")," or empty value does not perform any policy."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"FullPCPUsOnly")," requires that the scheduler must allocate full physical cores. Equivalent to kubelet CPU manager policy option ",(0,a.kt)("inlineCode",{parentName:"li"},"full-pcpus-only=true"),". ")),(0,a.kt)("p",null,"If there is no ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/cpu-bind-policy")," in the node's label, it will be executed according to the policy configured by the Pod or koord-scheduler."),(0,a.kt)("h4",{id:"numa-allocate-strategy"},"NUMA allocate strategy"),(0,a.kt)("p",null,"The label ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/numa-allocate-strategy")," indicates how to choose satisfied NUMA Nodes when scheduling. The following is the specific value definition:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"MostAllocated")," indicates that allocates from the NUMA Node with the least amount of available resource."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"LeastAllocated")," indicates that allocates from the NUMA Node with the most amount of available resource."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"DistributeEvenly")," indicates that evenly distribute CPUs across NUMA Nodes.")),(0,a.kt)("p",null,"If the cluster administrator does not set label ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/numa-allocate-strategy")," on Node, but ",(0,a.kt)("inlineCode",{parentName:"p"},"kubelet.koordinator.sh/cpu-manager-policy")," in ",(0,a.kt)("inlineCode",{parentName:"p"},"NodeResourceTopology")," has option ",(0,a.kt)("inlineCode",{parentName:"p"},"distribute-cpus-across-numa=true"),", then follow the semantic allocation of ",(0,a.kt)("inlineCode",{parentName:"p"},"distribute-cpus-across-numa"),". "),(0,a.kt)("p",null,"If there is no ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/numa-allocate-strategy")," in the node's label and no ",(0,a.kt)("inlineCode",{parentName:"p"},"kubelet.koordinator.sh/cpu-manager-policy")," with ",(0,a.kt)("inlineCode",{parentName:"p"},"distribute-cpus-across-numa")," option in ",(0,a.kt)("inlineCode",{parentName:"p"},"NodeResourceTopology"),", it will be executed according to the policy configured by the koord-scheduler."),(0,a.kt)("p",null,"If both ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/numa-allocate-strategy")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"kubelet.koordinator.sh/cpu-manager-policy")," are defined, ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/numa-allocate-strategy")," is used first."),(0,a.kt)("h4",{id:"numa-topology-alignment-policy"},"NUMA topology alignment policy"),(0,a.kt)("p",null,"The label ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/numa-topology-alignment-policy")," represents that how to aligning resource allocation according to the NUMA topology. The policy semantic follow the K8s community. Equivalent to the field ",(0,a.kt)("inlineCode",{parentName:"p"},"TopologyPolicies")," in ",(0,a.kt)("inlineCode",{parentName:"p"},"NodeResourceTopology"),", and the topology policies ",(0,a.kt)("inlineCode",{parentName:"p"},"SingleNUMANodePodLevel")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"SingleNUMANodeContainerLevel")," are mapping to ",(0,a.kt)("inlineCode",{parentName:"p"},"SingleNUMANode")," policy. "),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"None")," is the default policy and does not perform any topology alignment."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"BestEffort")," indicates that preferred select NUMA Node that is topology alignment, and if not, continue to allocate resources to Pods."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Restricted")," indicates that each resource requested by a Pod on the NUMA Node that is topology alignment, and if not, koord-scheduler will skip the node when scheduling."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"SingleNUMANode")," indicates that all resources requested by a Pod must be on the same NUMA Node, and if not, koord-scheduler will skip the node when scheduling.")),(0,a.kt)("p",null,"If there is no ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/numa-topology-alignment-policy")," in the node's label and ",(0,a.kt)("inlineCode",{parentName:"p"},"TopologyPolicies=None")," in ",(0,a.kt)("inlineCode",{parentName:"p"},"NodeResourceTopology"),", it will be executed according to the policy configured by the koord-scheduler."),(0,a.kt)("p",null,"If both ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/numa-topology-alignment-policy")," in Node and ",(0,a.kt)("inlineCode",{parentName:"p"},"TopologyPolicies=None")," in ",(0,a.kt)("inlineCode",{parentName:"p"},"NodeResourceTopology")," are defined, ",(0,a.kt)("inlineCode",{parentName:"p"},"node.koordinator.sh/numa-topology-alignment-policy")," is used first."),(0,a.kt)("h4",{id:"example-1"},"Example"),(0,a.kt)("p",null,"The following specific example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: Node\nmetadata:\n  labels:\n    node.koordinator.sh/cpu-bind-policy: "FullPCPUsOnly"\n    node.koordinator.sh/numa-topology-alignment-policy: "BestEffort"\n    node.koordinator.sh/numa-allocate-strategy: "MostAllocated"\n  name: node-0\nspec:\n  ...\n')),(0,a.kt)("h3",{id:"noderesourcetopology-crd"},"NodeResourceTopology CRD"),(0,a.kt)("p",null,"The node resource information to be reported mainly includes the following categories:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"NUMA Topology, including resources information, CPU information such as logical CPU ID, physical Core ID, NUMA Socket ID and NUMA Node ID and etc. "),(0,a.kt)("li",{parentName:"ul"},"The topology manager scopes and policies configured by kubelet."),(0,a.kt)("li",{parentName:"ul"},"The CPU manager policies and options configured by kubelet."),(0,a.kt)("li",{parentName:"ul"},"Pod bound CPUs allocated by kubelet or koord-scheduler, including K8s Guaranteed Pods, Koordinator LSE/LSR Pods but except the LS/BE."),(0,a.kt)("li",{parentName:"ul"},"CPU Shared Pool defined by koordlet")),(0,a.kt)("p",null,"The above information can guide koord-scheduler to better be compatible with the kubelet's CPU management logic, make more appropriate scheduling decisions and help users quickly troubleshoot."),(0,a.kt)("h4",{id:"crd-scheme-definition"},"CRD Scheme definition"),(0,a.kt)("p",null,"We use ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/k8stopologyawareschedwg/noderesourcetopology-api/blob/master/pkg/apis/topology/v1alpha1/types.go"},"NodeResourceTopology")," CRD to describe the NUMA Topology. The community-defined NodeResourceTopology CRD is mainly used for the following considerations:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"NodeResourceTopology already contains basic NUMA topology information and kubelet TopologyManager's Scope and Policies information. We can reuse the existing codes."),(0,a.kt)("li",{parentName:"ul"},"Keep up with the evolution of the community and influence the community to make more changes.")),(0,a.kt)("h4",{id:"compatible"},"Compatible"),(0,a.kt)("p",null,"In order to be compatible with the existing NodeResourceTopology instances and prevent koordlet from conflicting with existing components, when koordlet creates a NodeResourceTopology, add the prefix ",(0,a.kt)("inlineCode",{parentName:"p"},"koord-")," before the name to distinguish it, and add the label ",(0,a.kt)("inlineCode",{parentName:"p"},"app.kubernetes.io/managed-by=Koordinator")," describes the node is managed by Koordinator."),(0,a.kt)("h4",{id:"extension"},"Extension"),(0,a.kt)("p",null,"At present, the NodeResourceTopology lacks some information, and it is temporarily written in the NodeResourceTopology in the form of annotations or labels:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The annotation ",(0,a.kt)("inlineCode",{parentName:"li"},"kubelet.koordinator.sh/cpu-manger-policy")," describes the kubelet CPU manager policy and options. The scheme is defined as follows:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'const (\n  FullPCPUsOnlyOption            string = "full-pcpus-only"\n  DistributeCPUsAcrossNUMAOption string = "distribute-cpus-across-numa"\n)\n\ntype KubeletCPUManagerPolicy struct {\n  Policy  string            `json:"policy,omitempty"`\n  Options map[string]string `json:"options,omitempty"`\n}\n\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The annotation ",(0,a.kt)("inlineCode",{parentName:"li"},"node.koordinator.sh/cpu-topology")," describes the detailed CPU topology. Fine-grained management mechanism needs more detailed CPU topology information. The scheme is defined as follows:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'type CPUTopology struct {\n  Detail []CPUInfo `json:"detail,omitempty"`\n}\n\ntype CPUInfo struct {\n  ID     int32 `json:"id"`\n  Core   int32 `json:"core"`\n  Socket int32 `json:"socket"`\n  Node   int32 `json:"node"`\n}\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"annotation ",(0,a.kt)("inlineCode",{parentName:"li"},"node.koordinator.sh/pod-cpu-allocs")," describes the CPUs allocated by Koordinator LSE/LSR and K8s Guaranteed Pods. The scheme corresponding to the annotation value is defined as follows:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'type PodCPUAlloc struct {\n  Namespace string    `json:"namespace,omitempty"`\n  Name      string    `json:"name,omitempty"`\n  UID       types.UID `json:"uid,omitempty"`\n  CPUSet    string    `json:"cpuset,omitempty"`\n}\n\ntype PodCPUAllocs []PodCPUAlloc\n')),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The annotation ",(0,a.kt)("inlineCode",{parentName:"li"},"node.koordinator.sh/cpu-shared-pools")," describes the CPU Shared Pool defined by Koordinator. The shared pool is mainly used by Koordinator LS Pods or K8s Burstable Pods. The scheme is defined as follows:")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-go"},'type NUMACPUSharedPools []CPUSharedPool\n\ntype CPUSharedPool struct {\n  Socket int32  `json:"socket,omitempty"`\n  Node   int32  `json:"node,omitempty"`\n  CPUSet string `json:"cpuset,omitempty"`\n}\n')),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"CPUSet")," field is Linux CPU list formatted string. For more details, please refer to ",(0,a.kt)("a",{parentName:"p",href:"http://man7.org/linux/man-pages/man7/cpuset.7.html#FORMATS"},"doc"),"."),(0,a.kt)("h4",{id:"createupdate-noderesourcetopology"},"Create/Update NodeResourceTopology"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"koordlet is responsible for creating/updating NodeResourceTopology"),(0,a.kt)("li",{parentName:"ul"},"It is recommended that koordlet obtain the CPU allocation information of the existing K8s Guaranteed Pod by parsing the CPU state checkpoint file. Or obtain this information through the CRI interface and gRPC provided by kubelet."),(0,a.kt)("li",{parentName:"ul"},"When the CPU of the Pod is allocated by koord-scheduler, replace the CPUs in the kubelet state checkpoint file."),(0,a.kt)("li",{parentName:"ul"},"It is recommended that koordlet obtain the CPU manager policy and options from ",(0,a.kt)("a",{parentName:"li",href:"https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/"},"kubeletConfiguration"),".")),(0,a.kt)("h4",{id:"example-2"},"Example"),(0,a.kt)("p",null,"A complete NodeResourceTopology example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: topology.node.k8s.io/v1alpha1\nkind: NodeResourceTopology\nmetadata:\n  annotations:\n    kubelet.koordinator.sh/cpu-manger-policy: |-\n      {\n        "policy": "static",\n        "options": {\n          "full-pcpus-only": "true",\n          "distribute-cpus-across-numa": "true"\n        }\n      }\n    node.koordinator.sh/cpu-topology: |-\n          {\n            "detail": [\n              {\n                "id": 0,\n                "core": 0,\n                "socket": 0,\n                "node": 0\n              },\n              {\n                "id": 1,\n                "core": 1,\n                "socket": 1,\n                "node": 1\n              }\n            ]\n          }\n    node.koordinator.sh/cpu-shared-pools: |-\n      [\n        {\n          "socket": 0,\n          "node": 0, \n          "cpuset": "0-3"\n        }\n      ]\n    node.koordinator.sh/pod-cpu-allocs: |-\n      [\n        {\n          "namespace": "default",\n          "name": "test-pod",\n          "uid": "32b14702-2efe-4be9-a9da-f3b779175846",\n          "cpu": "4-8"\n        }\n      ]\n  labels:\n    app.kubernetes.io/managed-by: Koordinator\n  name: koord-node1\ntopologyPolicies: ["SingleNUMANodePodLevel"]\nzones:\n  - name: node-0\n    type: Node\n    resources:\n      - name: cpu\n        capacity: 20\n        allocatable: 15\n        available: 10\n      - name: vendor/nic1\n        capacity: 3\n        allocatable: 3\n        available: 3\n  - name: node-1\n    type: Node\n    resources:\n      - name: cpu\n        capacity: 30\n        allocatable: 25\n        available: 15\n      - name: vendor/nic2\n        capacity: 6\n        allocatable: 6\n        available: 6\n  - name: node-2\n    type: Node\n    resources:\n      - name: cpu\n        capacity: 30\n        allocatable: 25\n        available: 15\n      - name: vendor/nic1\n        capacity: 3\n        allocatable: 3\n        available: 3\n  - name: node-3\n    type: Node\n    resources:\n      - name: cpu\n        capacity: 30\n        allocatable: 25\n        available: 15\n      - name: vendor/nic1\n        capacity: 3\n        allocatable: 3\n        available: 3\n')))}h.isMDXComponent=!0},9252:function(e,o,t){o.Z=t.p+"assets/images/cpu-orchestration-seq-uml-196e3bd19e6260b22d063c22cdbf3213.svg"}}]);