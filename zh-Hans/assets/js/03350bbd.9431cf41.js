"use strict";(self.webpackChunkkoordinator_sh=self.webpackChunkkoordinator_sh||[]).push([[613],{3905:(e,o,n)=>{n.d(o,{Zo:()=>d,kt:()=>u});var t=n(7294);function a(e,o,n){return o in e?Object.defineProperty(e,o,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[o]=n,e}function r(e,o){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);o&&(t=t.filter((function(o){return Object.getOwnPropertyDescriptor(e,o).enumerable}))),n.push.apply(n,t)}return n}function l(e){for(var o=1;o<arguments.length;o++){var n=null!=arguments[o]?arguments[o]:{};o%2?r(Object(n),!0).forEach((function(o){a(e,o,n[o])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(o){Object.defineProperty(e,o,Object.getOwnPropertyDescriptor(n,o))}))}return e}function i(e,o){if(null==e)return{};var n,t,a=function(e,o){if(null==e)return{};var n,t,a={},r=Object.keys(e);for(t=0;t<r.length;t++)n=r[t],o.indexOf(n)>=0||(a[n]=e[n]);return a}(e,o);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)n=r[t],o.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var p=t.createContext({}),s=function(e){var o=t.useContext(p),n=o;return e&&(n="function"==typeof e?e(o):l(l({},o),e)),n},d=function(e){var o=s(e.components);return t.createElement(p.Provider,{value:o},e.children)},m="mdxType",c={inlineCode:"code",wrapper:function(e){var o=e.children;return t.createElement(t.Fragment,{},o)}},g=t.forwardRef((function(e,o){var n=e.components,a=e.mdxType,r=e.originalType,p=e.parentName,d=i(e,["components","mdxType","originalType","parentName"]),m=s(n),g=a,u=m["".concat(p,".").concat(g)]||m[g]||c[g]||r;return n?t.createElement(u,l(l({ref:o},d),{},{components:n})):t.createElement(u,l({ref:o},d))}));function u(e,o){var n=arguments,a=o&&o.mdxType;if("string"==typeof e||a){var r=n.length,l=new Array(r);l[0]=g;var i={};for(var p in o)hasOwnProperty.call(o,p)&&(i[p]=o[p]);i.originalType=e,i[m]="string"==typeof e?e:a,l[1]=i;for(var s=2;s<r;s++)l[s]=n[s];return t.createElement.apply(null,l)}return t.createElement.apply(null,n)}g.displayName="MDXCreateElement"},1288:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>p,contentTitle:()=>l,default:()=>c,frontMatter:()=>r,metadata:()=>i,toc:()=>s});var t=n(7462),a=(n(7294),n(3905));const r={},l="Job",i={unversionedId:"architecture/job",id:"architecture/job",title:"Job",description:"Job Scheduling",source:"@site/i18n/zh-Hans/docusaurus-plugin-content-docs/current/architecture/job.md",sourceDirName:"architecture",slug:"/architecture/job",permalink:"/zh-Hans/docs/next/architecture/job",draft:!1,editUrl:"https://github.com/koordinator-sh/koordinator.sh/edit/main/docs/architecture/job.md",tags:[],version:"current",lastUpdatedBy:"\u4e54\u666e",lastUpdatedAt:1760529207,formattedLastUpdatedAt:"2025\u5e7410\u670815\u65e5",frontMatter:{},sidebar:"docs",previous:{title:"QoS",permalink:"/zh-Hans/docs/next/architecture/qos"},next:{title:"GangScheduling",permalink:"/zh-Hans/docs/next/user-manuals/gang-scheduling"}},p={},s=[{value:"Job Scheduling",id:"job-scheduling",level:2},{value:"PodGroup",id:"podgroup",level:3},{value:"GangGroup",id:"ganggroup",level:3},{value:"Job-Level Preemption",id:"job-level-preemption",level:2},{value:"Preemption Algorithm",id:"preemption-algorithm",level:3},{value:"Preemption Reason for Victim",id:"preemption-reason-for-victim",level:3},{value:"Nominated Node for Preemptor",id:"nominated-node-for-preemptor",level:3},{value:"Network-Topology Aware",id:"network-topology-aware",level:2},{value:"Topology-Aware Scheduling Requirements",id:"topology-aware-scheduling-requirements",level:3},{value:"Cluster Network Topology",id:"cluster-network-topology",level:3},{value:"Network Topology Spec",id:"network-topology-spec",level:3},{value:"Network Topology Pod Index",id:"network-topology-pod-index",level:3},{value:"Topology Gather Algorithm",id:"topology-gather-algorithm",level:3}],d={toc:s},m="wrapper";function c(e){let{components:o,...r}=e;return(0,a.kt)(m,(0,t.Z)({},d,r,{components:o,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"job"},"Job"),(0,a.kt)("h2",{id:"job-scheduling"},"Job Scheduling"),(0,a.kt)("p",null,"A batch of pods that must be scheduled together is called a ",(0,a.kt)("inlineCode",{parentName:"p"},"Job"),". "),(0,a.kt)("h3",{id:"podgroup"},"PodGroup"),(0,a.kt)("p",null,"Sometimes, the batch of pods is completely homogeneous and only needs to accumulate to a specified minimum number before scheduling is successful. In this case, we can describe the ",(0,a.kt)("inlineCode",{parentName:"p"},"minMember")," through a separate ",(0,a.kt)("inlineCode",{parentName:"p"},"PodGroup"),", and then associate its ",(0,a.kt)("inlineCode",{parentName:"p"},"member")," pods through pod Labels. Here is a PodGroup with a minimum cumulative number of 2 and its ",(0,a.kt)("inlineCode",{parentName:"p"},"member")," pods."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-example\n  namespace: default\nspec:\n  minMember: 2\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: pod\nmetadata:\n  name: pod-example1\n  namespace: default\n  labels:\n    pod-group.scheduling.sigs.k8s.io: gang-example\nspec:\n  schedulerName: koord-scheduler\n  ...\n---\napiVersion: v1\nkind: pod\nmetadata:\n  name: pod-example2\n  namespace: default\n  labels:\n    pod-group.scheduling.sigs.k8s.io: gang-example\nspec:\n  schedulerName: koord-scheduler\n  ...\n")),(0,a.kt)("h3",{id:"ganggroup"},"GangGroup"),(0,a.kt)("p",null,"In other cases, the pods that must be scheduled together may not be homogeneous and must complete the minimum number of accumulations separately. In this case, Koordinator supports associating different ",(0,a.kt)("inlineCode",{parentName:"p"},"PodGroups")," to form a ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup")," through PodGroup Label. Here is a ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup")," with two PodGroups:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-example1\n  namespace: default\n  annotations:\n    gang.scheduling.koordinator.sh/groups: "[\\"default/gang-example1\\", \\"default/gang-example2\\"]"\nspec:\n  minMember: 1\n---\napiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-example2\n  namespace: default\n  annotations:\n    gang.scheduling.koordinator.sh/groups: "[\\"default/gang-example1\\", \\"default/gang-example2\\"]"\nspec:\n  minMember: 2\n')),(0,a.kt)("h2",{id:"job-level-preemption"},"Job-Level Preemption"),(0,a.kt)("p",null,"When a pod cannot be scheduled due to insufficient resources, Kube-Scheduler attempts to evict lower-priority pods to make room for it. This is traditional ",(0,a.kt)("strong",{parentName:"p"},"pod-level reemption"),". However, when a Job cannot be scheduled due to insufficient resources, the scheduler must ",(0,a.kt)("strong",{parentName:"p"},"make enough space for the entire Job to be scheduled"),". This type of preemption is called ",(0,a.kt)("strong",{parentName:"p"},"Job-level preemption"),"."),(0,a.kt)("h3",{id:"preemption-algorithm"},"Preemption Algorithm"),(0,a.kt)("p",null,"The job that initiates preemption is called the ",(0,a.kt)("inlineCode",{parentName:"p"},"preemptor"),", and the preempted pod is called the ",(0,a.kt)("inlineCode",{parentName:"p"},"victim"),". The overall workflow of job-level preemption is as follows:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Unschedulable pod \u2192 Enters PostFilter phase"),(0,a.kt)("li",{parentName:"ol"},"Is it a Job? \u2192 Yes \u2192 Fetch all member pods"),(0,a.kt)("li",{parentName:"ol"},"Check Job Preemption Eligibility:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"pods.spec.preemptionPolicy")," \u2260 Never"),(0,a.kt)("li",{parentName:"ul"},"No terminating victims on the currently nominated nodes of all member pods (prevent redundant preemption)"))),(0,a.kt)("li",{parentName:"ol"},"Find candidate nodes where preemption may help"),(0,a.kt)("li",{parentName:"ol"},"Perform dry-run to simulate removal of potential victims (low priority pods)"),(0,a.kt)("li",{parentName:"ol"},"Select optimal node + minimal-cost victim set (",(0,a.kt)("strong",{parentName:"li"},"job-aware cost model"),")"),(0,a.kt)("li",{parentName:"ol"},"Execute preemption:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Delete victims (by setting DisruptionTarget condition and invoking the deletion API)"),(0,a.kt)("li",{parentName:"ul"},"Clear ",(0,a.kt)("inlineCode",{parentName:"li"},"status.nominatedNode")," of other lower-priority nominated pods on the target nodes."),(0,a.kt)("li",{parentName:"ul"},"Set ",(0,a.kt)("inlineCode",{parentName:"li"},"status.nominatedNode")," for all member pods."))),(0,a.kt)("li",{parentName:"ol"},"Preemption successful \u2192 The pod enters the scheduling queue, waiting for victims to terminate.")),(0,a.kt)("h3",{id:"preemption-reason-for-victim"},"Preemption Reason for Victim"),(0,a.kt)("p",null,"When a victim is preempted, Koord-Scheduler adds an entry to ",(0,a.kt)("inlineCode",{parentName:"p"},"victim.status.conditions")," to indicate which job preempted it and triggers graceful termination. "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: pod\nmetadata:\n  name: victim-1\n  namespace: default\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: "2025-09-17T08:41:35Z"\n    message: \'koord-scheduler: preempting to accommodate higher priority pods, preemptor:\n      default/hello-job, triggerpod: default/preemptor-pod-2\'\n    reason: PreemptionByScheduler\n    status: "True"\n    type: DisruptionTarget\n')),(0,a.kt)("p",null,"The above shows that default/victim-1 was preempted by the high-priority job ",(0,a.kt)("inlineCode",{parentName:"p"},"hello-job"),". Member Pods of ",(0,a.kt)("inlineCode",{parentName:"p"},"hello-job")," can be retrieved via the following command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get po -n default -l pod-group.scheduling.sigs.k8s.io=hello-job\nhello-job-pod-1   0/1     Pending             0                5m\nhello-job-pod-2   0/1     Pending             0                5m\n")),(0,a.kt)("h3",{id:"nominated-node-for-preemptor"},"Nominated Node for Preemptor"),(0,a.kt)("p",null,"After a Job preemption succeeds, in addition to evicting the victim pods, the scheduler must also reserve the reclaimed resources in its internal cache. In Kubernetes, this is achieved using ",(0,a.kt)("inlineCode",{parentName:"p"},"pod.status.nominatedNode"),". In Koordinator, koord-scheduler sets the ",(0,a.kt)("inlineCode",{parentName:"p"},".status.nominatedNode")," field for ",(0,a.kt)("strong",{parentName:"p"},"all member pods of the preempting job")," to reflect this resource reservation."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: pod\nmetadata:\nname: preemptor-pod-1\nnamespace: default\nlabels:\n  pod-group.scheduling.sigs.k8s.io: hello-job\nstatus:\n  nominatednodeName: example-node\n  phase: Pending\n---\napiVersion: v1\nkind: pod\nmetadata:\n  name: preemptor-pod-2\n  namespace: default\n  labels:\n    pod-group.scheduling.sigs.k8s.io: hello-job\nstatus:\n  nominatednodeName: example-node\n  phase: Pending\n")),(0,a.kt)("p",null,"The above shows that the two pods of ",(0,a.kt)("inlineCode",{parentName:"p"},"hello-job")," have successfully completed preemption and are nominated for scheduling to example-node."),(0,a.kt)("h2",{id:"network-topology-aware"},"Network-Topology Aware"),(0,a.kt)("p",null,"In large-scale AI training scenarios, especially for large language models (LLMs), efficient inter-pod communication is critical to training performance. Model parallelism techniques such as Tensor Parallelism (TP), Pipeline Parallelism (PP), and Data Parallelism (DP) require frequent and high-bandwidth data exchange across GPUs\u2014often spanning multiple nodes. Under such workloads, network topology becomes a key performance bottleneck, where communication latency and bandwidth are heavily influenced by the physical ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/scheduling/20250611-networktopology-aware-scheduling.md#network-architecture"},"network hierarchy")," (e.g., NVLink, block, spine)."),(0,a.kt)("p",null,"To optimize training efficiency, ",(0,a.kt)("strong",{parentName:"p"},"pods within a ",(0,a.kt)("inlineCode",{parentName:"strong"},"GangGroup")," is required or preferred to be scheduled to nodes that reside in the same or nearby high-performance network domains"),", minimizing inter-node hops and maximizing throughput. For example, in a spine-block architecture, scheduling all member pods under the same ",(0,a.kt)("inlineCode",{parentName:"p"},"block")," or ",(0,a.kt)("inlineCode",{parentName:"p"},"spine")," switch significantly reduces communication latency compared to distributing them across different spines."),(0,a.kt)("h3",{id:"topology-aware-scheduling-requirements"},"Topology-Aware Scheduling Requirements"),(0,a.kt)("p",null,"While Kubernetes\u2019 native scheduler supports basic topology constraints via ",(0,a.kt)("inlineCode",{parentName:"p"},"PodAffinity"),", it operates on a per-Pod basis and lacks gang scheduling semantics, making it ineffective for coordinated placement of tightly coupled workloads. Koord-Scheduler abstracts ",(0,a.kt)("inlineCode",{parentName:"p"},"PodGroup")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup")," concept to providing all-or-nothing semantics, enabling collective scheduling of interdependent pods. Moreover, to meet the demands of modern AI training, we extend it with ",(0,a.kt)("strong",{parentName:"p"},"Network-Topology Aware Scheduling"),"\u2014a capability that intelligently selects optimal nodes based on network hierarchy."),(0,a.kt)("p",null,"This feature ensures:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"When cluster resources are sufficient, pods with network topology scheduling requirements will be scheduled to a topology domain with better performance (e.g., lower latency, higher bandwidth) according to user-specified strategies."),(0,a.kt)("li",{parentName:"ul"},"When cluster resources are insufficient, the scheduler will seize resources for the GangGroup based on network topology constraints through job-level preemption, and record the resource nominations in the ",(0,a.kt)("inlineCode",{parentName:"li"},".status.nominatedNode")," field to ensure consistent placement.")),(0,a.kt)("h3",{id:"cluster-network-topology"},"Cluster Network Topology"),(0,a.kt)("p",null,"Nodes are labeled with their network topology positions using tools like NVIDIA\u2019s ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/NVIDIA/topograph/blob/main/docs/k8s.md"},"topograph"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Node\nmetadata:\n  name: node-0\n  labels:\n    network.topology.nvidia.com/accelerator: nvl1\n    network.topology.nvidia.com/block: s1\n    network.topology.nvidia.com/spine: s2\n    network.topology.nvidia.com/datacenter: s3\n")),(0,a.kt)("p",null,"Administrators define the topology hierarchy via a ",(0,a.kt)("inlineCode",{parentName:"p"},"ClusterNetworkTopology")," CR named ",(0,a.kt)("inlineCode",{parentName:"p"},"default"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: scheduling.koordinator.sh/v1alpha1\nkind: ClusterNetworkTopology\nmetadata:\n  name: default\nspec:\n  networkTopologySpec:\n    - labelKey:\n      - network.topology.nvidia.com/spine\n      topologyLayer: SpineLayer\n    - labelKey:\n      - network.topology.nvidia.com/block\n      parentTopologyLayer: SpineLayer\n      topologyLayer: BlockLayer\n    - parentTopologyLayer: BlockLayer\n      topologyLayer: NodeTopologyLayer\n")),(0,a.kt)("p",null,"The topology forms a tree structure, where each layer represents a level of aggregation in the network (e.g., Node \u2192 block \u2192 spine)."),(0,a.kt)("p",null,"The ",(0,a.kt)("inlineCode",{parentName:"p"},"status.detailStatus")," field of ",(0,a.kt)("inlineCode",{parentName:"p"},"ClusterNetworkTopology")," is automatically maintained by Koordinator, reflecting the actual network topology structure and node distribution in the cluster. It presents a hierarchical view from the top-level (cluster) down to individual nodes. Each entry in ",(0,a.kt)("inlineCode",{parentName:"p"},"detailStatus")," represents an instance of a specific topology layer, with key fields:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"topologyInfo"),": The current layer's type and name (e.g., ",(0,a.kt)("inlineCode",{parentName:"li"},"SpineLayer"),", ",(0,a.kt)("inlineCode",{parentName:"li"},"s1"),")."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"parentTopologyInfo"),": The parent layer\u2019s information."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"childTopologyNames"),": List of child domains in the next lower layer."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"nodeNum"),": Number of nodes within this topology domain.")),(0,a.kt)("p",null,"The follwing is an example of ",(0,a.kt)("inlineCode",{parentName:"p"},"clusterNetworkTopology.status.detailStatus"),":"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: scheduling.koordinator.sh/v1alpha1\nkind: ClusterNetworkTopology\nmetadata:\n  name: default\nspec:\n  networkTopologySpec:\n  - labelKey:\n    - network.topology.nvidia.com/spine\n    topologyLayer: SpineLayer\n  - labelKey:\n    - network.topology.nvidia.com/block\n    parentTopologyLayer: SpineLayer\n    topologyLayer: BlockLayer\n  - parentTopologyLayer: BlockLayer\n    topologyLayer: NodeTopologyLayer\nstatus:\n  detailStatus:\n  - childTopologyLayer: SpineLayer\n    childTopologyNames:\n    - s1\n    - s2\n    nodeNum: 8\n    topologyInfo:\n      topologyLayer: ClusterTopologyLayer\n      topologyName: ""\n  - childTopologyLayer: BlockLayer\n    childTopologyNames:\n    - b2\n    - b1\n    nodeNum: 4\n    parentTopologyInfo:\n      topologyLayer: ClusterTopologyLayer\n      topologyName: ""\n    topologyInfo:\n      topologyLayer: SpineLayer\n      topologyName: s1\n  - childTopologyLayer: NodeTopologyLayer\n    nodeNum: 2\n    parentTopologyInfo:\n      topologyLayer: SpineLayer\n      topologyName: s1\n    topologyInfo:\n      topologyLayer: BlockLayer\n      topologyName: b2\n  - childTopologyLayer: NodeTopologyLayer\n    nodeNum: 2\n    parentTopologyInfo:\n      topologyLayer: SpineLayer\n      topologyName: s1\n    topologyInfo:\n      topologyLayer: BlockLayer\n      topologyName: b1\n  - childTopologyLayer: BlockLayer\n    childTopologyNames:\n    - b3\n    - b4\n    nodeNum: 4\n    parentTopologyInfo:\n      topologyLayer: ClusterTopologyLayer\n      topologyName: ""\n    topologyInfo:\n      topologyLayer: SpineLayer\n      topologyName: s2\n  - childTopologyLayer: NodeTopologyLayer\n    nodeNum: 2\n    parentTopologyInfo:\n      topologyLayer: SpineLayer\n      topologyName: s2\n    topologyInfo:\n      topologyLayer: BlockLayer\n      topologyName: b3\n  - childTopologyLayer: NodeTopologyLayer\n    nodeNum: 2\n    parentTopologyInfo:\n      topologyLayer: SpineLayer\n      topologyName: s2\n    topologyInfo:\n      topologyLayer: BlockLayer\n      topologyName: b4\n')),(0,a.kt)("p",null,"Based on the above ",(0,a.kt)("inlineCode",{parentName:"p"},"status"),", the cluster has a two-tier ",(0,a.kt)("strong",{parentName:"p"},"spine-block")," architecture:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre"},"ClusterTopologyLayer\n\u251c\u2500\u2500 SpineLayer: s1\n\u2502   \u251c\u2500\u2500 BlockLayer: b1\n\u2502   \u2502   \u2514\u2500\u2500 NodeTopologyLayer: 2 nodes\n\u2502   \u2514\u2500\u2500 BlockLayer: b2\n\u2502       \u2514\u2500\u2500 NodeTopologyLayer: 2 nodes\n\u2514\u2500\u2500 SpineLayer: s2\n    \u251c\u2500\u2500 BlockLayer: b3\n    \u2502   \u2514\u2500\u2500 NodeTopologyLayer: 2 nodes\n    \u2514\u2500\u2500 BlockLayer: b4\n        \u2514\u2500\u2500 NodeTopologyLayer: 2 nodes\n")),(0,a.kt)("h3",{id:"network-topology-spec"},"Network Topology Spec"),(0,a.kt)("p",null,"When users want to configure the network topology gather strategy for ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup"),", its ",(0,a.kt)("inlineCode",{parentName:"p"},"PodGroup")," can be annotated as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-master\n  namespace: default\n  annotations:\n    gang.scheduling.koordinator.sh/groups: ["default/gang-master", "default/gang-worker"]\n    gang.scheduling.koordinator.sh/network-topology-spec: |\n      {\n        "gatherStrategy": [\n          {\n            "layer": "SpineLayer",\n            "strategy": "PreferGather"\n          },\n          {\n            "layer": "BlockLayer",\n            "strategy": "PreferGather"\n          },\n          {\n            "layer": "AcceleratorLayer",\n            "strategy": "PreferGather"\n          }\n        ]\n      }\nspec:\n  minMember: 1\n--- \napiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-worker\n  namespace: default\n  annotations:\n    gang.scheduling.koordinator.sh/groups: ["default/gang-master", "default/gang-worker"]\n    gang.scheduling.koordinator.sh/network-topology-spec: |\n      {\n        "gatherStrategy": [\n          {\n            "layer": "SpineLayer",\n            "strategy": "PreferGather"\n          },\n          {\n            "layer": "BlockLayer",\n            "strategy": "PreferGather"\n          },\n          {\n            "layer": "AcceleratorLayer",\n            "strategy": "PreferGather"\n          }\n        ]\n      }\nspec:\n  minMember: 2\n')),(0,a.kt)("p",null,"The above ",(0,a.kt)("inlineCode",{parentName:"p"},"PodGroup")," indicates that the Pods belonging to it firstly try to be in an accelerator interconnection domain, and then try to be in a Block, and then try to be in a Spine network."),(0,a.kt)("p",null,"Sometimes, due to the strict demand for communication bandwidth, users may want to place all member Pods of a ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup")," under the same Spine. In this case, you can modify the ",(0,a.kt)("inlineCode",{parentName:"p"},"PodGroup")," as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-master\n  namespace: default\n  annotations:\n    gang.scheduling.koordinator.sh/groups: ["default/gang-master", "default/gang-worker"]\n    gang.scheduling.koordinator.sh/network-topology-spec: |\n      {\n        "gatherStrategy": [\n          {\n            "layer": "spineLayer",\n            "strategy": "MustGather"\n          }\n        ]\n      }\nspec:\n  minMember: 1\n---\napiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-worker\n  namespace: default\n  annotations:\n    gang.scheduling.koordinator.sh/groups: ["default/gang-master", "default/gang-worker"]\n    gang.scheduling.koordinator.sh/network-topology-spec: |\n      {\n        "gatherStrategy": [\n          {\n            "layer": "spineLayer",\n            "strategy": "MustGather"\n          }\n        ]\n      }\nspec:\n  minMember: 2\n')),(0,a.kt)("h3",{id:"network-topology-pod-index"},"Network Topology Pod Index"),(0,a.kt)("p",null,"In distributed training, assigning an index to each Pod is essential for establishing communication patterns in data-parallel (DP) job. The index determines the logical order of Pods in collective operations. For example, for a ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup")," with DP=2, the member pods can be annotated as:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example1\n  namespace: default\n  labels:\n    pod-group.scheduling.sigs.k8s.io: gang-example\n  annotations:\n    gang.scheduling.koordinator.sh/network-topology-index: "1"\nspec:\n  schedulerName: koord-scheduler\n...\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example2\n  namespace: default\n  labels:\n    pod-group.scheduling.sigs.k8s.io: gang-example\n  annotations:\n    gang.scheduling.koordinator.sh/network-topology-index: "2"\nspec:\n  schedulerName: koord-scheduler\n...\n')),(0,a.kt)("h3",{id:"topology-gather-algorithm"},"Topology Gather Algorithm"),(0,a.kt)("p",null,"The network topology gather algorithm is to find the best nodes for the M Pods, given the M member Pods belonging to a parallel-aware ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup"),", all the Nodes that can place the Pods, the network topology location of each node. The overall computation process can be described step by step as follows:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"The member Pods of the ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup")," of the training task are generally homogeneous. We randomly select one from the member Pods as the representative Pod.")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"From the bottom to the top of the network topology hierarchy, recursively calculate the number of member Pods that each topology node can provide as ",(0,a.kt)("inlineCode",{parentName:"p"},"offerslots"),". The ",(0,a.kt)("inlineCode",{parentName:"p"},"offerslots")," that a Node can accommodate can be achieved by iteratively calling ",(0,a.kt)("inlineCode",{parentName:"p"},"NodeInfo.AddPod"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"fwk.RunPreFilterExtensionsAddPod"),", and ",(0,a.kt)("inlineCode",{parentName:"p"},"fwk.RunFilterWithNominatedNode"),".")),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"Among all the topological nodes that can accommodate all the member Pods of the ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup"),", select those with the lowest level as our ",(0,a.kt)("inlineCode",{parentName:"p"},"candidate topological nodes"),". "),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"topology_offerslot_candidatenode",src:n(1403).Z,width:"3124",height:"1214"}))),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"Among the candidate topological nodes selected in 3, according to the ",(0,a.kt)("inlineCode",{parentName:"p"},"binpack")," principle, the candidate topological nodes whose offerslot is closest to the offerslot required by the job are selected as our final topological node solution. As shown in the figure below, we select Node5-Node8 as the final scheduling result of the job."),(0,a.kt)("p",{parentName:"li"},(0,a.kt)("img",{alt:"topology_final_placement",src:n(9835).Z,width:"2196",height:"820"})))))}c.isMDXComponent=!0},9835:(e,o,n)=>{n.d(o,{Z:()=>t});const t=n.p+"assets/images/topology_final_placement-1aa756976a2fb3e7699c4627250d6aaf.jpg"},1403:(e,o,n)=>{n.d(o,{Z:()=>t});const t=n.p+"assets/images/topology_offerslot_candidatenode-beea873b6973cb265ba7c56e0c3882a5.jpg"}}]);