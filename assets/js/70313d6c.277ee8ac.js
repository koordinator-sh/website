"use strict";(self.webpackChunkkoordinator_sh=self.webpackChunkkoordinator_sh||[]).push([[3979],{3905:(e,t,o)=>{o.d(t,{Zo:()=>u,kt:()=>m});var n=o(67294);function a(e,t,o){return t in e?Object.defineProperty(e,t,{value:o,enumerable:!0,configurable:!0,writable:!0}):e[t]=o,e}function i(e,t){var o=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),o.push.apply(o,n)}return o}function r(e){for(var t=1;t<arguments.length;t++){var o=null!=arguments[t]?arguments[t]:{};t%2?i(Object(o),!0).forEach((function(t){a(e,t,o[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(o)):i(Object(o)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(o,t))}))}return e}function s(e,t){if(null==e)return{};var o,n,a=function(e,t){if(null==e)return{};var o,n,a={},i=Object.keys(e);for(n=0;n<i.length;n++)o=i[n],t.indexOf(o)>=0||(a[o]=e[o]);return a}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)o=i[n],t.indexOf(o)>=0||Object.prototype.propertyIsEnumerable.call(e,o)&&(a[o]=e[o])}return a}var l=n.createContext({}),c=function(e){var t=n.useContext(l),o=t;return e&&(o="function"==typeof e?e(t):r(r({},t),e)),o},u=function(e){var t=c(e.components);return n.createElement(l.Provider,{value:t},e.children)},d="mdxType",h={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var o=e.components,a=e.mdxType,i=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),d=c(o),p=a,m=d["".concat(l,".").concat(p)]||d[p]||h[p]||i;return o?n.createElement(m,r(r({ref:t},u),{},{components:o})):n.createElement(m,r({ref:t},u))}));function m(e,t){var o=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var i=o.length,r=new Array(i);r[0]=p;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[d]="string"==typeof e?e:a,r[1]=s;for(var c=2;c<i;c++)r[c]=o[c];return n.createElement.apply(null,r)}return n.createElement.apply(null,o)}p.displayName="MDXCreateElement"},93025:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var n=o(87462),a=(o(67294),o(3905));const i={slug:"release-v1.4.0",title:"Koordinator v1.4: more types of computing workloads and more flexible resource management mechanisms",authors:["ZiMengSheng"],tags:["release"]},r=void 0,s={permalink:"/blog/release-v1.4.0",editUrl:"https://github.com/koordinator-sh/koordinator.sh/edit/main/blog/2024-01-15-release/index.md",source:"@site/blog/2024-01-15-release/index.md",title:"Koordinator v1.4: more types of computing workloads and more flexible resource management mechanisms",description:"Background",date:"2024-01-15T00:00:00.000Z",formattedDate:"January 15, 2024",tags:[{label:"release",permalink:"/blog/tags/release"}],readingTime:19.125,hasTruncateMarker:!1,authors:[{name:"Jianyu Wang",title:"Koordinator approver",url:"https://github.com/ZiMengSheng",imageURL:"https://github.com/ZiMengSheng.png",key:"ZiMengSheng"}],frontMatter:{slug:"release-v1.4.0",title:"Koordinator v1.4: more types of computing workloads and more flexible resource management mechanisms",authors:["ZiMengSheng"],tags:["release"]},prevItem:{title:"Koordinator v1.5: continuous optimization, join CNCF Sandbox",permalink:"/blog/release-v1.5.0"},nextItem:{title:"Koordinator v1.3: \u589e\u5f3a\u8d44\u6e90\u9884\u7559\uff0c\u652f\u6301 NRI\uff0c\u63d0\u4f9b\u8282\u70b9\u753b\u50cf\u7684 Mid \u8d44\u6e90\u8d85\u5356",permalink:"/blog/release-v1.3.0"}},l={authorsImageUrls:[void 0]},c=[{value:"Background",id:"background",level:2},{value:"Interpretation of Version Features",id:"interpretation-of-version-features",level:2},{value:"1. Support Kubernetes and YARN workload co-location",id:"1-support-kubernetes-and-yarn-workload-co-location",level:3},{value:"2. Introducing NUMA topology alignment strategy",id:"2-introducing-numa-topology-alignment-strategy",level:3},{value:"3. ElasticQuota evolves again",id:"3-elasticquota-evolves-again",level:3},{value:"3.1 Introducing Multi QuotaTree",id:"31-introducing-multi-quotatree",level:4},{value:"3.2 Support non-preemptible",id:"32-support-non-preemptible",level:4},{value:"3.3 Other improvements",id:"33-other-improvements",level:4},{value:"4. CPU normalization",id:"4-cpu-normalization",level:3},{value:"5. Improved descheduling protection strategies",id:"5-improved-descheduling-protection-strategies",level:3},{value:"6. Cold Memory reporting",id:"6-cold-memory-reporting",level:3},{value:"7. QoS management for non-containerized applications",id:"7-qos-management-for-non-containerized-applications",level:3},{value:"8. Other features",id:"8-other-features",level:3},{value:"Future plan",id:"future-plan",level:2},{value:"Conclusion",id:"conclusion",level:2}],u={toc:c},d="wrapper";function h(e){let{components:t,...i}=e;return(0,a.kt)(d,(0,n.Z)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h2",{id:"background"},"Background"),(0,a.kt)("p",null,"As an actively developing open source project, Koordinator has undergo multiple version iterations since the release of v0.1.0 in April 2022, continuously bringing innovations and enhancements to the Kubernetes ecosystem. The core objective of the project is to provide comprehensive solutions for orchestrating collocated workloads, scheduling resources, ensuring resource isolation, and tuning performance to help users optimize container performance and improve cluster resource utilization."),(0,a.kt)("p",null,"In past version iterations, the Koordinator community has continued to grow, receiving active participation and contributions from engineers at well-known companies. These include Alibaba, Ant Technology Group, Intel, Xiaomi, Xiaohongshu, iQIYI, Qihoo 360, Youzan, Quwan, Meiya Pico, PITS, among others. Each version has advanced through the collective efforts of the community, demonstrating the project's capability to address challenges in actual production environments."),(0,a.kt)("p",null,"Today, we are pleased to announce that Koordinator v1.4.0 is officially released. This version introduces several new features, including Kubernetes and YARN workload co-location, a NUMA topology alignment strategy, CPU normalization, and cold memory reporting. It also enhances features in key areas such as elastic quota management, QoS management for non-containerized applications on hosts, and descheduling protection strategies. These innovations and improvements aim to better support enterprise-level Kubernetes cluster environments, particularly in complex and diverse application scenarios."),(0,a.kt)("p",null,"The release of version v1.4.0 will bring users support for more types of computing workloads and more flexible resource management mechanisms. We look forward to these improvements helping users to address a broader range of enterprise resource management challenges. In the v1.4.0 release, a total of 11 new developers have joined the development of the Koordinator community. They are @shaloulcy, @baowj-678, @zqzten, @tan90github, @pheianox, @zxh326, @qinfustu, @ikaven1024, @peiqiaoWang, @bogo-y, and @xujihui1985.  We thank all community members for their active participation and contributions during this period and for their ongoing commitment to the community."),(0,a.kt)("h2",{id:"interpretation-of-version-features"},"Interpretation of Version Features"),(0,a.kt)("h3",{id:"1-support-kubernetes-and-yarn-workload-co-location"},"1. Support Kubernetes and YARN workload co-location"),(0,a.kt)("p",null,"Koordinator already supports the co-location of online and offline workloads within the Kubernetes ecosystem. However, outside the Kubernetes ecosystem, a considerable number of big data workloads still run on traditional Hadoop YARN."),(0,a.kt)("p",null,"In response, the Koordinator community, together with developers from Alibaba Cloud, Xiaohongshu, and Ant Financial, has jointly launched the Hadoop YARN and Kubernetes co-location project, Koordinator YARN Copilot. This initiative enables the running of Hadoop NodeManager within Kubernetes clusters, fully leveraging the technical value of peak-shaving and resource reuse for different types of workloads. Koordinator YARN Copilot has the following features:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Embracing the open-source ecosystem: Built upon the open-source version of Hadoop YARN without any intrusive modifications to YARN."),(0,a.kt)("li",{parentName:"ul"},"Unified resource priority and QoS policy: YARN NodeManager utilizes Koordinator\u2019s Batch priority resources and adheres to Koordinator's QoS management policies."),(0,a.kt)("li",{parentName:"ul"},"Node-level resource sharing: The co-location resources provided by Koordinator can be used by both Kubernetes pod and YARN tasks. Different types of offline applications can run on the same node.")),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"img",src:o(81749).Z,width:"396",height:"451"})),(0,a.kt)("p",null,"For the detailed design of Koordinator YARN Copilot and its use in the Xiaohongshu production environment, please refer to ",(0,a.kt)("a",{parentName:"p",href:"https://mp.weixin.qq.com/s/N0QEJYyOhoDZoVQ6hGhnmg"},"Previous Articles")," and ",(0,a.kt)("a",{parentName:"p",href:"https://koordinator.sh/zh-Hans/docs/next/designs/koordinator-yarn"},"Official Community Documents"),"."),(0,a.kt)("h3",{id:"2-introducing-numa-topology-alignment-strategy"},"2. Introducing NUMA topology alignment strategy"),(0,a.kt)("p",null,"The workloads running in Kubernetes clusters are increasingly diverse, particularly in fields such as machine learning, where the demand for high-performance computing resources is on the rise. In these fields, a significant amount of CPU resources is required, as well as other high-speed computing resources like GPUs and RDMA. Moreover, to achieve optimal performance, these resources often need to be located on the same NUMA node or even the same PCIe bus."),(0,a.kt)("p",null,"Kubernetes' kubelet includes a topology manager that manages the NUMA topology of resource allocation. It attempts to align the topologies of multiple resources at the node level during the admission phase. However, because the node component lacks a global view of the scheduler and the timing of node selection for pods, pods may be scheduled on nodes that are unable to meet the topology alignment policy. This can result in pods failing to start due to ",(0,a.kt)("inlineCode",{parentName:"p"},"topology affinity")," errors."),(0,a.kt)("p",null,"To solve this problem, Koordinator moves NUMA topology selection and alignment to the central scheduler, optimizing resource NUMA topology at the cluster level. In this release, Koordinator introduces NUMA-aware scheduling of CPU resources (including Batch resources) and NUMA-aware scheduling of GPU devices as alpha features. The entire suite of NUMA-aware scheduling features is rapidly evolving."),(0,a.kt)("p",null,"Koordinator enables users to configure the NUMA topology alignment strategy for multiple resources on a node through the node's labels. The configurable strategies are as follows:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"None"),", the default strategy, does not perform any topological alignment."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"BestEffort")," indicates that the node does not strictly allocate resources according to NUMA topology alignment. The scheduler can always allocate such nodes to pods as long as the remaining resources meet the pods' needs."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"Restricted")," means that nodes allocate resources in strict accordance with NUMA topology alignment. In other words, the scheduler must select the same one or more NUMA nodes when allocating multiple resources, otherwise, the node should not be considered. For instance, if a pod requests 33 CPU cores and each NUMA node has 32 cores, it can be allocated to use two NUMA nodes. Moreover, if the pod also requests GPUs or RDMA, these must be on the same NUMA node as the CPU."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"SingleNUMANode")," is similar to ",(0,a.kt)("inlineCode",{parentName:"li"},"Restricted"),", adhering strictly to NUMA topology alignment, but it differs in that ",(0,a.kt)("inlineCode",{parentName:"li"},"Restricted")," permits the use of multiple NUMA nodes, whereas ",(0,a.kt)("inlineCode",{parentName:"li"},"SingleNUMANode")," restricts allocation to a single NUMA node.")),(0,a.kt)("p",null,"For example, to set the ",(0,a.kt)("inlineCode",{parentName:"p"},"SingleNUMANode")," policy for node-0, you would do the following:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: Node\nmetadata:\n  labels:\n    node.koordinator.sh/numa-topology-policy: "SingleNUMANode"\n  name: node-0\nspec:\n  ...\n')),(0,a.kt)("p",null,"In a production environment, users may have enabled kubelet's topology alignment policy, which will be reflected by the koordlet in the TopologyPolicies field of the NodeResourceTopology CR object. When kubelet's policy conflicts with the policy set by the user on the node, the kubelet policy shall take precedence. The koord-scheduler essentially adopts the same NUMA alignment policy semantics as the kubelet topology manager. The kubelet policies SingleNUMANodePodLevel and SingleNUMANodeContainerLevel are both mapped to SingleNUMANode."),(0,a.kt)("p",null,"After configuring the NUMA alignment strategy for the node, the scheduler can identify many suitable NUMA node allocation results for each pod. Koordinator currently provides the NodeNUMAResource plugin, which allows for configuring the NUMA node allocation result scoring strategy for CPU and memory resources. This includes ",(0,a.kt)("inlineCode",{parentName:"p"},"LeastAllocated")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"MostAllocated")," strategies, with ",(0,a.kt)("inlineCode",{parentName:"p"},"LeastAllocated")," being the default. Each resource can also be assigned a configurable weight. The scheduler will ultimately select the NUMA node allocation with the highest score. For instance, we can configure the NUMA node allocation result scoring strategy to ",(0,a.kt)("inlineCode",{parentName:"p"},"MostAllocated"),", as shown in the following example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: kubescheduler.config.k8s.io/v1beta2\nkind: KubeSchedulerConfiguration\nprofiles:\n  - pluginConfig:\n      - name: NodeNUMAResource\n        args:\n          apiVersion: kubescheduler.config.k8s.io/v1beta2\n          kind: NodeNUMAResourceArgs\n          scoringStrategy:  # Here configure Node level scoring strategy\n            type: MostAllocated\n            resources:\n              - name: cpu\n                weight: 1\n              - name: memory\n                weight: 1\n              - name: "kubernetes.io/batch-cpu"\n                weight: 1\n              - name: "kubernetes.io/batch-memory"\n                weight: 1\n          numaScoringStrategy: # Here configure NUMA-Node level scoring strategy\n            type: MostAllocated\n            resources:\n              - name: cpu\n                weight: 1\n              - name: memory\n                weight: 1\n              - name: "kubernetes.io/batch-cpu"\n                weight: 1\n              - name: "kubernetes.io/batch-memory"\n                weight: 1\n')),(0,a.kt)("h3",{id:"3-elasticquota-evolves-again"},"3. ElasticQuota evolves again"),(0,a.kt)("p",null,"In order to fully utilize cluster resources and reduce management system costs, users often deploy workloads from multiple tenants in the same cluster. When cluster resources are limited, competition for these resources is inevitable between different tenants. As a result, the workloads of some tenants may always be satisfied, while others may never be executed, leading to demands for fairness. The quota mechanism is a very natural way to ensure fairness among tenants, where each tenant is allocated a specific quota, and they can use resources within that quota. Tasks exceeding the quota will not be scheduled or executed. However, simple quota management cannot fulfill tenants' expectations for elasticity in the cloud. Users hope that in addition to satisfying resource requests within the quota, requests for resources beyond the quota can also be met on demand."),(0,a.kt)("p",null,"In previous versions, Koordinator leveraged the upstream ElasticQuota protocol, which allowed tenants to set a 'Min' value to express their resource requests that must be satisfied, and a 'Max' value to limit the maximum resources they can use. 'Max' was also used to represent the shared weight of the remaining resources of the cluster when they were insufficient."),(0,a.kt)("p",null,"In addition to offering a flexible quota mechanism that accommodates tenants' on-demand resource requests, Koordinator enhances ElasticQuota with annotations to organize it into a tree structure, thereby simplifying the expression of hierarchical organizational structures for users."),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"img",src:o(76344).Z,width:"1216",height:"792"})),(0,a.kt)("p",null,"The figure above depicts a common quota tree in a cluster utilizing Koordinator's elastic quota. The root quota serves as the link between the quota system and the actual resources within the cluster. In previous iterations, the root quota existed only within the scheduler's logic. In this release, we have made the root quota accessible to users in the form of a Custom Resource (CR). Users can now view information about the root quota through the ElasticQuota CR named ",(0,a.kt)("inlineCode",{parentName:"p"},"koordinator-root-quota"),"."),(0,a.kt)("h4",{id:"31-introducing-multi-quotatree"},"3.1 Introducing Multi QuotaTree"),(0,a.kt)("p",null,"In large clusters, there are various types of nodes. For example, VMs provided by cloud vendors will have different architectures. The most common ones are amd64 and arm64. There are also different models with the same architecture. In addition, nodes generally have location attributes such as availability zone. When nodes of different types are managed in the same quota tree, their unique attributes will be lost. When users want to manage the unique attributes of machines in a refined manner, the current ElasticQuota appears not to be accurate enough. In order to meet users' requirements for flexible resource management or resource isolation, Koordinator supports users to divide the resources in the cluster into multiple parts, each part is managed by a quota tree, as shown in the following figure:"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"img",src:o(71249).Z,width:"1502",height:"634"})),(0,a.kt)("p",null,"Additionally, to help users simplify management complexity, Koordinator introduced the ElasticQuotaProfile mechanism in version 1.4.0. Users can quickly associate nodes with different quota trees through the nodeSelector, as shown in the following example:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: quota.koordinator.sh/v1alpha1\nkind: ElasticQuotaProfile\nmetadata:\n  labels:\n    kubernetes.io/arch: amd64\n  name: amd64-profile\n  namespace: kube-system\nspec:\n  nodeSelector:\n    matchLabels:\n      kubernetes.io/arch: amd64 // amd64 node\n  quotaName: amd64-root-quota   // the name of root quota\n---\napiVersion: quota.koordinator.sh/v1alpha1\nkind: ElasticQuotaProfile\nmetadata:\n  labels:\n    kubernetes.io/arch: arm64   \n  name: arm64-profile\n  namespace: kube-system\nspec:\n  nodeSelector:\n    matchLabels:\n      kubernetes.io/arch: arm64  // arm64 node\n  quotaName: arm64-root-quota    // the name of root quota\n")),(0,a.kt)("p",null,"After associating nodes with the quota tree, the user utilizes the ElasticQuota in each quota tree as before. When a user submits a pod to the corresponding quota, they currently still need to configure the pod's NodeAffinity to ensure that the pod runs on the correct node. In the future, we plan to add a feature that will help users automatically manage the mapping relationship from quota to node."),(0,a.kt)("h4",{id:"32-support-non-preemptible"},"3.2 Support non-preemptible"),(0,a.kt)("p",null,"Koordinator ElasticQuota supports sharing the unused part of 'Min' in ElasticQuota with other ElasticQuotas to improve resource utilization. However, when resources are tight, the pod that borrows the quota will be preempted and evicted through the preemption mechanism to get the resources back."),(0,a.kt)("p",null,"In actual production environments, if some critical online services borrow this part of the quota from other ElasticQuotas and preemption subsequently occurs, the quality of service may be adversely affected. Such workloads should not be subject to preemption."),(0,a.kt)("p",null,"To implement this safeguard, Koordinator v1.4.0 introduced a new API. Users can simply annotate a pod with ",(0,a.kt)("inlineCode",{parentName:"p"},"quota.scheduling.koordinator.sh/preemptible: false")," to indicate that the pod should not be preempted."),(0,a.kt)("p",null,"When the scheduler detects that a pod is declared non-preemptible, it ensures that the available quota for such a pod does not exceed its 'Min'. Thus, it is important to note that when enabling this feature, the 'Min' of an ElasticQuota should be set judiciously, and the cluster must have appropriate resource guarantees in place. This feature maintains compatibility with the original behavior of Koordinator."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\n  namespace: default\n  labels:\n    quota.scheduling.koordinator.sh/name: "quota-example"\n    quota.scheduling.koordinator.sh/preemptible: false\nspec:\n...\n')),(0,a.kt)("h4",{id:"33-other-improvements"},"3.3 Other improvements"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"The koord-scheduler previously supported the use of a single ElasticQuota object across multiple namespaces. However, in some cases, it is desirable for the same object to be shared by only a select few namespaces. To accommodate this need, users can now annotate the ElasticQuota CR with ",(0,a.kt)("inlineCode",{parentName:"li"},"quota.scheduling.koordinator.sh/namespaces"),", assigning a JSON string array as the value."),(0,a.kt)("li",{parentName:"ol"},"Performance optimization: Previously, whenever an ElasticQuota was modified, the ElasticQuota plugin would rebuild the entire quota tree. This process has been optimized in version 1.4.0."),(0,a.kt)("li",{parentName:"ol"},"Support ignoring overhead: When a pod utilizes secure containers, an overhead declaration is typically added to the pod specification to account for the resource consumption of the secure container itself. However, whether these additional resource costs should be passed on to the end user depends on the resource pricing strategy. If it is expected that users should not be responsible for these costs, the ElasticQuota can be configured to disregard overhead. With version 1.4.0, this can be achieved by enabling the feature gate ",(0,a.kt)("inlineCode",{parentName:"li"},"ElasticQuotaIgnorePodOverhead"),".")),(0,a.kt)("h3",{id:"4-cpu-normalization"},"4. CPU normalization"),(0,a.kt)("p",null,"With the diversification of node hardware in Kubernetes clusters, significant performance differences exist between CPUs of various architectures and generations. Therefore, even if a pod's CPU request is identical, the actual computing power it receives can vary greatly, potentially leading to resource waste or diminished application performance. The objective of CPU normalization is to ensure that each CPU unit in Kubernetes provides consistent computing power across heterogeneous nodes by standardizing the performance of allocatable CPUs."),(0,a.kt)("p",null,"To address this issue, Koordinator has implemented a CPU normalization mechanism in version 1.4.0. This mechanism adjusts the amount of CPU resources that can be allocated on a node according to the node's resource amplification strategy, ensuring that each allocatable CPU in the cluster delivers a consistent level of computing power. The overall architecture is depicted in the figure below:"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"img",src:o(75642).Z,width:"917",height:"481"})),(0,a.kt)("p",null,"CPU normalization consists of two steps"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"CPU performance evaluation: To calculate the performance benchmarks of different CPUs, you can refer to the industrial performance evaluation standard, ",(0,a.kt)("a",{parentName:"li",href:"https://www.spec.org/cpu2017/"},"SPEC CPU"),". This part is not provided by the Koordinator project."),(0,a.kt)("li",{parentName:"ol"},"Configuration of the CPU normalization ratio in Koordinator: The scheduling system schedules resources based on the normalization ratio, which is provided by Koordinator.")),(0,a.kt)("p",null,"Configure the CPU normalization ratio information into ",(0,a.kt)("inlineCode",{parentName:"p"},"slo-controller-config")," of koord-manager. The configuration example is as follows:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: slo-controller-config\n  namespace: koordinator-system\ndata:\n  cpu-normalization-config: |\n    {\n      "enable": true,\n      "ratioModel": {\n         "Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz": {\n           "baseRatio": 1.29,\n           "hyperThreadEnabledRatio": 0.82,\n           "turboEnabledRatio": 1.52,\n           "hyperThreadTurboEnabledRatio": 1.0\n         },\n         "Intel Xeon Platinum 8369B CPU @ 2.90GHz": {\n           "baseRatio": 1.69,\n           "hyperThreadEnabledRatio": 1.06,\n           "turboEnabledRatio": 1.91,\n           "hyperThreadTurboEnabledRatio": 1.20\n         }\n      }\n    }\n  # ...\n')),(0,a.kt)("p",null,"For nodes configured with CPU normalization, Koordinator intercepts updates to Node.Status.Allocatable by kubelet through a webhook to achieve the amplification of CPU resources. This results in the display of the normalized amount of CPU resources available for allocation on the node."),(0,a.kt)("h3",{id:"5-improved-descheduling-protection-strategies"},"5. Improved descheduling protection strategies"),(0,a.kt)("p",null,"Pod migration is a complex process that involves steps such as auditing, resource allocation, and application startup. It is often intertwined with application upgrades, scaling scenarios, and the resource operations and maintenance performed by cluster administrators. Consequently, if a large number of pods are migrated simultaneously, the system's stability may be compromised. Furthermore, migrating many pods from the same workload at once can also affect the application's stability. Additionally, simultaneous migrations of pods from multiple jobs may lead to a 'thundering herd' effect. Therefore, it is preferable to process the pods in each job sequentially."),(0,a.kt)("p",null,"To address these issues, Koordinator previously provided the PodMigrationJob function with some protection strategies. In version v1.4.0, Koordinator has enhanced these protection strategies into an arbitration mechanism. When there is a large number of executable PodMigrationJobs, the arbiter decides which ones can proceed by employing sorting and filtering techniques."),(0,a.kt)("p",null,"The sorting process is as follows:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The time interval between the start of migration and the current, the smaller the interval, the higher the ranking."),(0,a.kt)("li",{parentName:"ul"},"The pod priority of PodMigrationJob, the lower the priority, the higher the ranking."),(0,a.kt)("li",{parentName:"ul"},"Disperse Jobs by workload, make PodMigrationJobs close in the same job."),(0,a.kt)("li",{parentName:"ul"},"If some pods in the job containing PodMigrationJob's pod is being migrated, the PodMigrationJob's ranking is higher.")),(0,a.kt)("p",null,"The filtering process is as follows:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Group and filter PodMigrationJobs based on workload, node, namespace, etc."),(0,a.kt)("li",{parentName:"ul"},"Check the number of running podMigrationJobs in each workload, and those that reach a certain threshold will be excluded."),(0,a.kt)("li",{parentName:"ul"},"Check whether the number of unavailable replicas in each workload exceeds the maximum number of unavailable replicas, and those that exceed the number will be excluded."),(0,a.kt)("li",{parentName:"ul"},"Check whether the number of pods being migrated on the node where the target pod is located exceeds the maximum migration amount of a single node, and those that exceed will be excluded.")),(0,a.kt)("h3",{id:"6-cold-memory-reporting"},"6. Cold Memory reporting"),(0,a.kt)("p",null,"To improve system performance, the kernel generally tries not to free the page cache requested by an application but allocates as much as possible to the application. Although allocated by the kernel, this memory may no longer be accessed by applications and is referred to as cold memory."),(0,a.kt)("p",null,"Koordinator introduced the cold memory reporting function in version 1.4, primarily to lay the groundwork for future cold memory recycling capabilities. Cold memory recycling is designed to address two scenarios:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"In standard Kubernetes clusters, when the node memory level is too high, sudden memory requests can lead to direct memory recycling of the system. This can affect the performance of running containers and, in extreme cases, may result in out-of-memory (OOM) events if recycling is not timely. Therefore, maintaining a relatively free pool of node memory resources is crucial for runtime stability."),(0,a.kt)("li",{parentName:"ol"},"In co-location scenarios, high-priority applications' unused requested resources can be recycled by lower-priority applications. Since memory not reclaimed by the operating system is invisible to the Koordinator scheduling system, reclaiming unused memory pages of a container is beneficial for improving resource utilization.")),(0,a.kt)("p",null,"Koordlet has added a cold page collector to its collector plugins for reading the cgroup file ",(0,a.kt)("inlineCode",{parentName:"p"},"memory.idle_stat"),", which is exported by kidled (Anolis kernel), kstaled (Google), or DAMON (Amazon). This file contains information about cold pages in the page cache and is present at every hierarchy level of memory. Koordlet already supports the kidled cold page collector and provides interfaces for other cold page collectors."),(0,a.kt)("p",null,"After collecting cold page information, the cold page collector stores the metrics, such as hot page usage and cold page size for nodes, pods, and containers, into metriccache. This data is then reported to the NodeMetric Custom Resource (CR)."),(0,a.kt)("p",null,"Users can enable cold memory recycling and configure cold memory collection strategies through NodeMetric. Currently, three strategies are offered: ",(0,a.kt)("inlineCode",{parentName:"p"},"usageWithHotPageCache"),", ",(0,a.kt)("inlineCode",{parentName:"p"},"usageWithoutPageCache")," and ",(0,a.kt)("inlineCode",{parentName:"p"},"usageWithPageCache"),". For more details, please see the community ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/koordinator-sh/koordinator/blob/main/docs/proposals/koordlet/20230728-support-cold-memory-compute.md"},"Design Document"),"\u3002"),(0,a.kt)("h3",{id:"7-qos-management-for-non-containerized-applications"},"7. QoS management for non-containerized applications"),(0,a.kt)("p",null,"In the process of enterprise containerization, there may be non-containerized applications running on the host alongside those already running on Kubernetes. In order to be better compatible with enterprises in the containerization process, Koordinator has developed a node resource reservation mechanism. This mechanism can reserve resources and assign specific QoS (Quality of Service) levels to applications that have not yet been containerized. Unlike the resource reservation configuration provided by kubelet, Koordinator's primary goal is to address QoS issues that arise during the runtime of both non-containerized and containerized applications. The overall solution is depicted in the figure below:"),(0,a.kt)("p",null,(0,a.kt)("img",{alt:"img",src:o(75051).Z,width:"406",height:"241"})),(0,a.kt)("p",null,"Currently, applications need to start processes into the corresponding cgroup according to specifications, and Koordinator does not provide an automatic cgroup relocation tool. For host non-containerized applications, QoS is supported as follows:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"LS (Latency Sensitive)")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"CPU QoS (Group Identity): The application runs the process in the CPU subsystem of the cgroup according to the specification, and the koordlet sets the Group Identity parameter for it according to the CPU QoS configuration;"))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"CPUSet Allocation: The application runs the process in the CPU subsystem of the cgroup according to the specification, and the koordlet will set all CPU cores in the CPU share pool for it."))),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("p",{parentName:"li"},"BE (Best-effort)")),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"CPU QoS (Group Identity): The application runs the process in the CPU subsystem of the cgroup according to the specification, and the koordlet sets the Group Identity parameter for it according to the configuration of CPU QoS.")))),(0,a.kt)("p",null,"For detailed design of QoS management of non-containerized applications on the host, please refer to ",(0,a.kt)("a",{parentName:"p",href:"https://koordinator.sh/zh-Hans/docs/next/user-manuals/host-application-qos"},"Community Documentation"),". In the future, we will gradually add support for other QoS strategies for host non-containerized applications."),(0,a.kt)("h3",{id:"8-other-features"},"8. Other features"),(0,a.kt)("p",null,"In addition to the new features and functional enhancements mentioned above, Koordinator has also implemented the following bug fixes and optimizations in version 1.4.0:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"RequiredCPUBindPolicy: Fine-grained CPU orchestration now supports the configuration of the required CPU binding policy, which means that CPUs are allocated strictly in accordance with the specified CPU binding policy; otherwise, scheduling will fail."),(0,a.kt)("li",{parentName:"ol"},"CICD: The Koordinator community provides a set of e2e testing Pipeline in v1.4.0; an ARM64 image is provided."),(0,a.kt)("li",{parentName:"ol"},"Batch resource calculation strategy optimization: There is support for the ",(0,a.kt)("inlineCode",{parentName:"li"},"maxUsageRequest")," calculation strategy, which conservatively reclaims high-priority resources. This update also optimizes the underestimate of Batch allocatable when a large number of pods start and stop on a node in a short period of time and improves considerations for special circumstances such as host non-containerized application, third-party allocatable, and dangling pod usage."),(0,a.kt)("li",{parentName:"ol"},"Others: Optimizations include using libpfm4 and perf groups to improve CPI collection, allowing SystemResourceCollector to support customized expiration time configuration, enabling BE pods to calculate CPU satisfaction based on the evictByAllocatable policy, repairing koordlet's CPUSetAllocator filtering logic for pods with LS and None QoS, and enhancing RDT resource control to retrieve the task IDs of sandbox containers.")),(0,a.kt)("p",null,"For a comprehensive list of new features in version 1.4.0, please visit the ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/koordinator-sh/koordinator/releases/tag/v1.4.0"},"v1.4.0 Release")," page."),(0,a.kt)("h2",{id:"future-plan"},"Future plan"),(0,a.kt)("p",null,"In upcoming versions, Koordinator has planned the following features:"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Core Scheduling: On the runtime side, Koordinator has begun exploring the next generation of CPU QoS capabilities. By leveraging kernel mechanisms such as Linux Core Scheduling, it aims to enhance resource isolation at the physical core level and reduce the security risks associated with co-location. For more details on this work, see ",(0,a.kt)("a",{parentName:"li",href:"https://github.com/koordinator-sh/koordinator/issues/1728"},"Issue #1728"),"."),(0,a.kt)("li",{parentName:"ul"},"Joint Allocation of Devices: In scenarios involving AI large model distributed training, GPUs from different machines often need to communicate through high-performance network cards. Performance is improved when GPUs and high-performance network cards are allocated in close proximity. Koordinator is advancing the joint allocation of multiple heterogeneous resources. Currently, it supports joint allocation in terms of protocol and scheduler logic; the reporting logic for network card resources on the node side is being explored.")),(0,a.kt)("p",null,"For more information, please pay attention to ",(0,a.kt)("a",{parentName:"p",href:"https://github.com/koordinator-sh/koordinator/milestone/14"},"Milestone v1.5.0"),"."),(0,a.kt)("h2",{id:"conclusion"},"Conclusion"),(0,a.kt)("p",null,"Finally, we are immensely grateful to all the contributors and users of the Koordinator community. Your active participation and valuable advice have enabled Koordinator to continue improving. We eagerly look forward to your ongoing feedback and warmly welcome new contributors to join our ranks."))}h.isMDXComponent=!0},75642:(e,t,o)=>{o.d(t,{Z:()=>n});const n=o.p+"assets/images/cpu-normalization-42902e00c064a6a32c368a92720dca8e.svg"},81749:(e,t,o)=>{o.d(t,{Z:()=>n});const n=o.p+"assets/images/hadoop-k8s-a092bf3c9bc72245fec2b31b173a8792.svg"},75051:(e,t,o)=>{o.d(t,{Z:()=>n});const n=o.p+"assets/images/host-application-101f6caa7664a1a2cc38c6d8cf01a6a1.svg"},71249:(e,t,o)=>{o.d(t,{Z:()=>n});const n=o.p+"assets/images/multiquotatree-9680fdd40b65082d7f63092f830c8995.png"},76344:(e,t,o)=>{o.d(t,{Z:()=>n});const n=o.p+"assets/images/quotatree1-b68ec769c15547410ee4e5bd6eec1b27.jpg"}}]);