"use strict";(self.webpackChunkkoordinator_sh=self.webpackChunkkoordinator_sh||[]).push([[5456],{3905:(e,t,a)=>{a.d(t,{Zo:()=>m,kt:()=>h});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var d=n.createContext({}),s=function(e){var t=n.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},m=function(e){var t=s(e.components);return n.createElement(d.Provider,{value:t},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},c=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,d=e.parentName,m=l(e,["components","mdxType","originalType","parentName"]),p=s(a),c=r,h=p["".concat(d,".").concat(c)]||p[c]||u[c]||i;return a?n.createElement(h,o(o({ref:t},m),{},{components:a})):n.createElement(h,o({ref:t},m))}));function h(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=c;var l={};for(var d in t)hasOwnProperty.call(t,d)&&(l[d]=t[d]);l.originalType=e,l[p]="string"==typeof e?e:r,o[1]=l;for(var s=2;s<i;s++)o[s]=a[s];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}c.displayName="MDXCreateElement"},77171:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var n=a(87462),r=(a(67294),a(3905));const i={},o="Device Scheduling - GPU Share With HAMi",l={unversionedId:"user-manuals/device-scheduling-gpu-share-with-hami",id:"user-manuals/device-scheduling-gpu-share-with-hami",title:"Device Scheduling - GPU Share With HAMi",description:"Introduction",source:"@site/docs/user-manuals/device-scheduling-gpu-share-with-hami.md",sourceDirName:"user-manuals",slug:"/user-manuals/device-scheduling-gpu-share-with-hami",permalink:"/docs/next/user-manuals/device-scheduling-gpu-share-with-hami",draft:!1,editUrl:"https://github.com/koordinator-sh/koordinator.sh/edit/main/docs/user-manuals/device-scheduling-gpu-share-with-hami.md",tags:[],version:"current",lastUpdatedBy:"qinfustu",lastUpdatedAt:1762917857,formattedLastUpdatedAt:"Nov 12, 2025",frontMatter:{},sidebar:"docs",previous:{title:"Device Scheduling - Basics",permalink:"/docs/next/user-manuals/fine-grained-device-scheduling"},next:{title:"GPU & RDMA Joint Allocation",permalink:"/docs/next/user-manuals/gpu-and-rdma-joint-allocation"}},d={},s=[{value:"Introduction",id:"introduction",level:2},{value:"Setup",id:"setup",level:2},{value:"Prerequisite",id:"prerequisite",level:3},{value:"Installation",id:"installation",level:3},{value:"Runtime Requirements",id:"runtime-requirements",level:4},{value:"Configurations",id:"configurations",level:3},{value:"Install hami-daemon with helms",id:"install-hami-daemon-with-helms",level:4},{value:"Options: chart parameters",id:"options-chart-parameters",level:5},{value:"Use GPU Share With HAMi",id:"use-gpu-share-with-hami",level:2},{value:"View vGPU monitoring",id:"view-vgpu-monitoring",level:2},{value:"FAQ",id:"faq",level:2}],m={toc:s},p="wrapper";function u(e){let{components:t,...i}=e;return(0,r.kt)(p,(0,n.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"device-scheduling---gpu-share-with-hami"},"Device Scheduling - GPU Share With HAMi"),(0,r.kt)("h2",{id:"introduction"},"Introduction"),(0,r.kt)("p",null,"GPU is an indispensable device for today's large AI model training and inference, and can provide powerful computing power support for AI applications. An NVIDIA A100 80 GB GPU can provide up to 249 times the inference performance on the BERT-LARGE task compared to a CPU. However, behind such powerful computing power is an expensive price. An NVIDIA A100 40GB model GPU chip is priced at around $13,000, while an A100 80GB model GPU chip is priced at around $15,000. We have observed that some inference tasks often use less than a full GPU, but only use, for example, 50% of the computing power or gpu memory. Therefore, sharing a GPU with multiple Pods can significantly improve GPU resource utilization."),(0,r.kt)("p",null,(0,r.kt)("a",{parentName:"p",href:"https://project-hami.io/docs/developers/hami-core-design/"},"HAMi")," is a ",(0,r.kt)("a",{parentName:"p",href:"https://cncf.io/"},"Cloud Native Computing Foundation")," sandbox project which provides the ability to share Heterogeneous AI devices among tasks. Koordinator takes advantage of HAMi's GPU isolation capabilities on the node side to provide an end-to-end GPU sharing solution. It includes two key components:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"HAMi-core")," is the in-container gpu resource controller, it operates by Hijacking the API-call between CUDA-Runtime(libcudart.so) and CUDA-Driver(libcuda.so). The output from building HAMi-core is libvgpu.so. "),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("strong",{parentName:"li"},"vGPUmonitor")," is a vGPU monitoring service provided by HAMi. HAMi-core writes vGPU monitoring data to the /usr/local/vgpu/containers directory, and vGPUmonitor parses the data in this directory to export Prometheus metrics.")),(0,r.kt)("h2",{id:"setup"},"Setup"),(0,r.kt)("h3",{id:"prerequisite"},"Prerequisite"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Kubernetes >= 1.18"),(0,r.kt)("li",{parentName:"ul"},"Koordinator >= 1.7")),(0,r.kt)("h3",{id:"installation"},"Installation"),(0,r.kt)("p",null,"Please make sure Koordinator components are correctly installed in your cluster. If not, please refer to ",(0,r.kt)("a",{parentName:"p",href:"/docs/installation"},"Installation"),"."),(0,r.kt)("h4",{id:"runtime-requirements"},"Runtime Requirements"),(0,r.kt)("p",null,"The scheduled GPU devices are bound to the container requires support from the runtime environment. Currently, there are two solutions to achieve this:"),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Runtime Environment"),(0,r.kt)("th",{parentName:"tr",align:null},"Installation"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"Containerd >= 1.7.0 ",(0,r.kt)("br",null)," Koordinator >= 1.7"),(0,r.kt)("td",{parentName:"tr",align:null},"Please make sure NRI is enabled in containerd. If not, please refer to ",(0,r.kt)("a",{parentName:"td",href:"https://github.com/containerd/containerd/blob/main/docs/NRI.md"},"Enable NRI in Containerd"))))),(0,r.kt)("h3",{id:"configurations"},"Configurations"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"DeviceScheduling is ",(0,r.kt)("em",{parentName:"li"},"Enabled")," by default. You can use it without any modification on the koord-scheduler config."),(0,r.kt)("li",{parentName:"ol"},"The GPUEnvInject FeatureFG need to be enabled in koordlet. Add ",(0,r.kt)("inlineCode",{parentName:"li"},"-feature-gates=GPUEnvInject=true")," to the args parameters.")),(0,r.kt)("h4",{id:"install-hami-daemon-with-helms"},"Install hami-daemon with helms"),(0,r.kt)("p",null,"hami-daemon can be simply installed by helm v3.5+, which is a simple command-line tool, and you can get it from ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/helm/helm/releases"},"here"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},"# Firstly add koordinator charts repository if you haven't do this.\n$ helm repo add koordinator-sh https://koordinator-sh.github.io/charts/\n\n# [Optional]\n$ helm repo update\n\n# Install the latest version.\n$ helm install hami-daemon koordinator-sh/hami-daemon --version 0.1.0\n")),(0,r.kt)("h5",{id:"options-chart-parameters"},"Options: chart parameters"),(0,r.kt)("p",null,"Note that installing this chart directly means it will use the default template values for hami-daemon."),(0,r.kt)("p",null,"You may have to set your specific configurations if it is deployed into a production cluster."),(0,r.kt)("p",null,"The following table lists the configurable parameters of the chart and their default values."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Parameter"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"),(0,r.kt)("th",{parentName:"tr",align:null},"Default"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"daemon.affinity")),(0,r.kt)("td",{parentName:"tr",align:null},"Affinity policy for the hami-daemon pod"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"Please view the values.yaml file in the chart"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"daemon.tolerations")),(0,r.kt)("td",{parentName:"tr",align:null},"Toleration policy for the hami-daemon pod"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"Please view the values.yaml file in the chart"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"daemon.hamiCore.libvgpuSoVersion")),(0,r.kt)("td",{parentName:"tr",align:null},"The version number of the libvgpu.so file"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"v2.6.0"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"daemon.hamiCore.image.repository")),(0,r.kt)("td",{parentName:"tr",align:null},"Repository for hami image"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"docker.m.daocloud.io/projecthami/hami"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"daemon.hamiCore.image.tag")),(0,r.kt)("td",{parentName:"tr",align:null},"Tag for hami image"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"v2.6.0"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"daemon.hamiCore.imagePullPolicy")),(0,r.kt)("td",{parentName:"tr",align:null},"Image pull policy"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"IfNotPresent"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"daemon.hamiCore.resources")),(0,r.kt)("td",{parentName:"tr",align:null},"HAMi resource request quantity"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"Please view the values.yaml file in the chart"))),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"daemon.vgpuMonitor.enabled")),(0,r.kt)("td",{parentName:"tr",align:null},"Whether to enable vGPU monitor"),(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"true"))))),(0,r.kt)("p",null,"Specify each parameter using the ",(0,r.kt)("inlineCode",{parentName:"p"},"--set key=value[,key=value]")," argument to ",(0,r.kt)("inlineCode",{parentName:"p"},"helm install")," or ",(0,r.kt)("inlineCode",{parentName:"p"},"helm upgrade"),"."),(0,r.kt)("h2",{id:"use-gpu-share-with-hami"},"Use GPU Share With HAMi"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Create a Pod to apply for a GPU card with 50% computing power and 50% gpu memory, and specify the need for hami-core isolation through the Pod Label ",(0,r.kt)("inlineCode",{parentName:"li"},"koordinator.sh/gpu-isolation-provider=HAMi-core"),". ",(0,r.kt)("strong",{parentName:"li"},"Starting from version 1.7.0, the scheduler will enforce a mandatory check: for any Pod requiring GPU isolation capabilities, its target node must have the hami-core component installed. Please ensure that all nodes in the cluster with hami-core installed are labeled with ",(0,r.kt)("inlineCode",{parentName:"strong"},"koordinator.sh/gpu-isolation-provider=HAMi-core"),"."))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\n  namespace: default\n  labels:\n    koordinator.sh/gpu-isolation-provider: HAMi-core\nspec:\n  schedulerName: koord-scheduler\n  containers:\n  - command:\n    - sleep\n    - 365d\n    image: nvidia/cuda:11.8.0-base-ubuntu22.04\n    imagePullPolicy: IfNotPresent\n    name: curlimage\n    resources:\n      limits:\n        cpu: 4\n        memory: 2Gi\n        koordinator.sh/gpu.shared: 1\n        koordinator.sh/gpu-core: 50\n        koordinator.sh/gpu-memory-ratio: 50\n      requests:\n        cpu: 4\n        memory: 2Gi\n        koordinator.sh/gpu.shared: 1\n        koordinator.sh/gpu-core: 50\n        koordinator.sh/gpu-memory-ratio: 50\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n  restartPolicy: Always\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get pod -n default pod-example -o yaml\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    scheduling.koordinator.sh/device-allocated: \'{"gpu":[{"minor":1,"resources":{"koordinator.sh/gpu-core":"50","koordinator.sh/gpu-memory":"11520Mi","koordinator.sh/gpu-memory-ratio":"50"}}]}\'\n  name: pod-example\n  namespace: default\n  labels:\n    koordinator.sh/gpu-isolation-provider: HAMi-core\n...\n')),(0,r.kt)("p",null,"You can find the concrete device allocate result through annotation ",(0,r.kt)("inlineCode",{parentName:"p"},"scheduling.koordinator.sh/device-allocated"),"."),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Enter the pod and you can see that the upper limit of the gpu memory seen by the program inside the pod is the value shown in the allocation result above.")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl exec -it -n default pod-example bash\n")),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:a(89014).Z,width:"1014",height:"460"})),(0,r.kt)("h2",{id:"view-vgpu-monitoring"},"View vGPU monitoring"),(0,r.kt)("p",null,"If the vgpuMonitor is enabled during the installation of hami-daemon and a default Prometheus is present in the cluster, then vGPU-related metrics can be queried from Prometheus."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Metrics Name"),(0,r.kt)("th",{parentName:"tr",align:null},"Description"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"HostGPUMemoryUsage")),(0,r.kt)("td",{parentName:"tr",align:null},"GPU device memory usage")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"HostCoreUtilization")),(0,r.kt)("td",{parentName:"tr",align:null},"GPU core utilization")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"vGPU_device_memory_usage_in_bytes")),(0,r.kt)("td",{parentName:"tr",align:null},"vGPU device usage")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"vGPU_device_memory_limit_in_bytes")),(0,r.kt)("td",{parentName:"tr",align:null},"vGPU device limit")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"Device_memory_desc_of_container")),(0,r.kt)("td",{parentName:"tr",align:null},"Container device memory description")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"Device_utilization_desc_of_container")),(0,r.kt)("td",{parentName:"tr",align:null},"Container device utilization description")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},(0,r.kt)("inlineCode",{parentName:"td"},"Device_last_kernel_of_container")),(0,r.kt)("td",{parentName:"tr",align:null},"Container device last kernel description")))),(0,r.kt)("p",null,"For example, accessing port 9394 (",(0,r.kt)("inlineCode",{parentName:"p"},"curl {pod-ip}:9394/metrics"),") of a hami-daemon pod allows you to retrieve relevant metrics, as shown below:"),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"image",src:a(39027).Z,width:"1788",height:"544"})),(0,r.kt)("h2",{id:"faq"},"FAQ"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/koordinator-sh/koordinator/issues/2517"},"Drivers above 570 need v2.6.0 to support"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},(0,r.kt)("a",{parentName:"p",href:"https://github.com/koordinator-sh/koordinator/issues/2620"},"HAMi-Core \u548c MPS \u51b2\u7a81")))))}u.isMDXComponent=!0},39027:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/gpu-monitor-result-4ec023a92646f9b5dc9df088c55f20b8.jpg"},89014:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/gpu-share-with-hami-result-492c0b4534ad611c9fb7aa019db15d42.png"}}]);