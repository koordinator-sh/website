"use strict";(self.webpackChunkkoordinator_sh=self.webpackChunkkoordinator_sh||[]).push([[3082],{3905:(e,t,n)=>{n.d(t,{Zo:()=>d,kt:()=>h});var o=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);t&&(o=o.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,o)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,o,a=function(e,t){if(null==e)return{};var n,o,a={},r=Object.keys(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(o=0;o<r.length;o++)n=r[o],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var p=o.createContext({}),s=function(e){var t=o.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},d=function(e){var t=s(e.components);return o.createElement(p.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return o.createElement(o.Fragment,{},t)}},c=o.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,p=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),m=s(n),c=a,h=m["".concat(p,".").concat(c)]||m[c]||u[c]||r;return n?o.createElement(h,i(i({ref:t},d),{},{components:n})):o.createElement(h,i({ref:t},d))}));function h(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=c;var l={};for(var p in t)hasOwnProperty.call(t,p)&&(l[p]=t[p]);l.originalType=e,l[m]="string"==typeof e?e:a,i[1]=l;for(var s=2;s<r;s++)i[s]=n[s];return o.createElement.apply(null,i)}return o.createElement.apply(null,n)}c.displayName="MDXCreateElement"},5275:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>i,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>s});var o=n(7462),a=(n(7294),n(3905));const r={},i="Job",l={unversionedId:"architecture/job",id:"architecture/job",title:"Job",description:"Job Scheduling",source:"@site/docs/architecture/job.md",sourceDirName:"architecture",slug:"/architecture/job",permalink:"/docs/next/architecture/job",draft:!1,editUrl:"https://github.com/koordinator-sh/koordinator.sh/edit/main/docs/architecture/job.md",tags:[],version:"current",lastUpdatedBy:"\u4e54\u666e",lastUpdatedAt:1758631965,formattedLastUpdatedAt:"Sep 23, 2025",frontMatter:{},sidebar:"docs",previous:{title:"QoS",permalink:"/docs/next/architecture/qos"},next:{title:"GangScheduling",permalink:"/docs/next/user-manuals/gang-scheduling"}},p={},s=[{value:"Job Scheduling",id:"job-scheduling",level:2},{value:"PodGroup",id:"podgroup",level:3},{value:"GangGroup",id:"ganggroup",level:3},{value:"Job-Level Preemption",id:"job-level-preemption",level:2},{value:"Preemption Algorithm",id:"preemption-algorithm",level:3},{value:"Preemption Reason for Victim",id:"preemption-reason-for-victim",level:3},{value:"Nominated Node for Preemptor",id:"nominated-node-for-preemptor",level:3}],d={toc:s},m="wrapper";function u(e){let{components:t,...n}=e;return(0,a.kt)(m,(0,o.Z)({},d,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"job"},"Job"),(0,a.kt)("h2",{id:"job-scheduling"},"Job Scheduling"),(0,a.kt)("p",null,"A batch of pods that must be scheduled together is called a ",(0,a.kt)("inlineCode",{parentName:"p"},"Job"),". "),(0,a.kt)("h3",{id:"podgroup"},"PodGroup"),(0,a.kt)("p",null,"Sometimes, the batch of pods is completely homogeneous and only needs to accumulate to a specified minimum number before scheduling is successful. In this case, we can describe the ",(0,a.kt)("inlineCode",{parentName:"p"},"minMember")," through a separate ",(0,a.kt)("inlineCode",{parentName:"p"},"PodGroup"),", and then associate its ",(0,a.kt)("inlineCode",{parentName:"p"},"member")," pods through pod Labels. Here is a PodGroup with a minimum cumulative number of 2 and its ",(0,a.kt)("inlineCode",{parentName:"p"},"member")," pods."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-example\n  namespace: default\nspec:\n  minMember: 2\n")),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: pod\nmetadata:\n  name: pod-example1\n  namespace: default\n  labels:\n    pod-group.scheduling.sigs.k8s.io: gang-example\nspec:\n  schedulerName: koord-scheduler\n  ...\n---\napiVersion: v1\nkind: pod\nmetadata:\n  name: pod-example2\n  namespace: default\n  labels:\n    pod-group.scheduling.sigs.k8s.io: gang-example\nspec:\n  schedulerName: koord-scheduler\n  ...\n")),(0,a.kt)("h3",{id:"ganggroup"},"GangGroup"),(0,a.kt)("p",null,"In other cases, the pods that must be scheduled together may not be homogeneous and must complete the minimum number of accumulations separately. In this case, Koordinator supports associating different ",(0,a.kt)("inlineCode",{parentName:"p"},"PodGroups")," to form a ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup")," through PodGroup Label. Here is a ",(0,a.kt)("inlineCode",{parentName:"p"},"GangGroup")," with two PodGroups:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-example1\n  namespace: default\n  annotations:\n    gang.scheduling.koordinator.sh/groups: "[\\"default/gang-example1\\", \\"default/gang-example2\\"]"\nspec:\n  minMember: 1\n---\napiVersion: scheduling.sigs.k8s.io/v1alpha1\nkind: PodGroup\nmetadata:\n  name: gang-example2\n  namespace: default\n  annotations:\n    gang.scheduling.koordinator.sh/groups: "[\\"default/gang-example1\\", \\"default/gang-example2\\"]"\nspec:\n  minMember: 2\n')),(0,a.kt)("h2",{id:"job-level-preemption"},"Job-Level Preemption"),(0,a.kt)("p",null,"When a pod cannot be scheduled due to insufficient resources, Kube-Scheduler attempts to evict lower-priority pods to make room for it. This is traditional ",(0,a.kt)("strong",{parentName:"p"},"pod-level reemption"),". However, when a Job cannot be scheduled due to insufficient resources, the scheduler must ",(0,a.kt)("strong",{parentName:"p"},"make enough space for the entire Job to be scheduled"),". This type of preemption is called ",(0,a.kt)("strong",{parentName:"p"},"Job-level preemption"),"."),(0,a.kt)("h3",{id:"preemption-algorithm"},"Preemption Algorithm"),(0,a.kt)("p",null,"The job that initiates preemption is called the ",(0,a.kt)("inlineCode",{parentName:"p"},"preemptor"),", and the preempted pod is called the ",(0,a.kt)("inlineCode",{parentName:"p"},"victim"),". The overall workflow of job-level preemption is as follows:"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},"Unschedulable pod \u2192 Enters PostFilter phase"),(0,a.kt)("li",{parentName:"ol"},"Is it a Job? \u2192 Yes \u2192 Fetch all member pods"),(0,a.kt)("li",{parentName:"ol"},"Check Job Preemption Eligibility:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"pods.spec.preemptionPolicy")," \u2260 Never"),(0,a.kt)("li",{parentName:"ul"},"No terminating victims on the currently nominated nodes of all member pods (prevent redundant preemption)"))),(0,a.kt)("li",{parentName:"ol"},"Find candidate nodes where preemption may help"),(0,a.kt)("li",{parentName:"ol"},"Perform dry-run to simulate removal of potential victims (low priority pods)"),(0,a.kt)("li",{parentName:"ol"},"Select optimal node + minimal-cost victim set (",(0,a.kt)("strong",{parentName:"li"},"job-aware cost model"),")"),(0,a.kt)("li",{parentName:"ol"},"Execute preemption:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Delete victims (by setting DisruptionTarget condition and invoking the deletion API)"),(0,a.kt)("li",{parentName:"ul"},"Clear ",(0,a.kt)("inlineCode",{parentName:"li"},"status.nominatedNode")," of other lower-priority nominated pods on the target nodes."),(0,a.kt)("li",{parentName:"ul"},"Set ",(0,a.kt)("inlineCode",{parentName:"li"},"status.nominatedNode")," for all member pods."))),(0,a.kt)("li",{parentName:"ol"},"Preemption successful \u2192 The pod enters the scheduling queue, waiting for victims to terminate.")),(0,a.kt)("h3",{id:"preemption-reason-for-victim"},"Preemption Reason for Victim"),(0,a.kt)("p",null,"When a victim is preempted, Koord-Scheduler adds an entry to ",(0,a.kt)("inlineCode",{parentName:"p"},"victim.status.conditions")," to indicate which job preempted it and triggers graceful termination. "),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},'apiVersion: v1\nkind: pod\nmetadata:\n  name: victim-1\n  namespace: default\nstatus:\n  conditions:\n  - lastProbeTime: null\n    lastTransitionTime: "2025-09-17T08:41:35Z"\n    message: \'koord-scheduler: preempting to accommodate higher priority pods, preemptor:\n      default/hello-job, triggerpod: default/preemptor-pod-2\'\n    reason: PreemptionByScheduler\n    status: "True"\n    type: DisruptionTarget\n')),(0,a.kt)("p",null,"The above shows that default/victim-1 was preempted by the high-priority job ",(0,a.kt)("inlineCode",{parentName:"p"},"hello-job"),". Member Pods of ",(0,a.kt)("inlineCode",{parentName:"p"},"hello-job")," can be retrieved via the following command:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get po -n default -l pod-group.scheduling.sigs.k8s.io=hello-job\nhello-job-pod-1   0/1     Pending             0                5m\nhello-job-pod-2   0/1     Pending             0                5m\n")),(0,a.kt)("h3",{id:"nominated-node-for-preemptor"},"Nominated Node for Preemptor"),(0,a.kt)("p",null,"After a Job preemption succeeds, in addition to evicting the victim pods, the scheduler must also reserve the reclaimed resources in its internal cache. In Kubernetes, this is achieved using ",(0,a.kt)("inlineCode",{parentName:"p"},"pod.status.nominatedNode"),". In Koordinator, koord-scheduler sets the ",(0,a.kt)("inlineCode",{parentName:"p"},".status.nominatedNode")," field for ",(0,a.kt)("strong",{parentName:"p"},"all member pods of the preempting job")," to reflect this resource reservation."),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: pod\nmetadata:\nname: preemptor-pod-1\nnamespace: default\nlabels:\n  pod-group.scheduling.sigs.k8s.io: hello-job\nstatus:\n  nominatednodeName: example-node\n  phase: Pending\n---\napiVersion: v1\nkind: pod\nmetadata:\n  name: preemptor-pod-2\n  namespace: default\n  labels:\n    pod-group.scheduling.sigs.k8s.io: hello-job\nstatus:\n  nominatednodeName: example-node\n  phase: Pending\n")),(0,a.kt)("p",null,"The above shows that the two pods of ",(0,a.kt)("inlineCode",{parentName:"p"},"hello-job")," have successfully completed preemption and are nominated for scheduling to example-node."))}u.isMDXComponent=!0}}]);