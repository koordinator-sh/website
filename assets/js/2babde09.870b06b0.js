"use strict";(self.webpackChunkkoordinator_sh=self.webpackChunkkoordinator_sh||[]).push([[4851],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>h});var a=t(7294);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function s(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,a,o=function(e,n){if(null==e)return{};var t,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(o[t]=e[t]);return o}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var i=a.createContext({}),d=function(e){var n=a.useContext(i),t=n;return e&&(t="function"==typeof e?e(n):s(s({},n),e)),t},c=function(e){var n=d(e.components);return a.createElement(i.Provider,{value:n},e.children)},p="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,o=e.mdxType,r=e.originalType,i=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=d(t),m=o,h=p["".concat(i,".").concat(m)]||p[m]||u[m]||r;return t?a.createElement(h,s(s({ref:n},c),{},{components:t})):a.createElement(h,s({ref:n},c))}));function h(e,n){var t=arguments,o=n&&n.mdxType;if("string"==typeof e||o){var r=t.length,s=new Array(r);s[0]=m;var l={};for(var i in n)hasOwnProperty.call(n,i)&&(l[i]=n[i]);l.originalType=e,l[p]="string"==typeof e?e:o,s[1]=l;for(var d=2;d<r;d++)s[d]=t[d];return a.createElement.apply(null,s)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},6481:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>i,contentTitle:()=>s,default:()=>u,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var a=t(7462),o=(t(7294),t(3905));const r={},s="Resource Reservation",l={unversionedId:"user-manuals/resource-reservation",id:"user-manuals/resource-reservation",title:"Resource Reservation",description:"Resource Reservation is an ability of koord-scheduler for reserving node resources for specific pods or workloads.",source:"@site/docs/user-manuals/resource-reservation.md",sourceDirName:"user-manuals",slug:"/user-manuals/resource-reservation",permalink:"/docs/next/user-manuals/resource-reservation",draft:!1,editUrl:"https://github.com/koordinator-sh/koordinator.sh/edit/main/docs/user-manuals/resource-reservation.md",tags:[],version:"current",lastUpdatedBy:"saintube",lastUpdatedAt:1760579818,formattedLastUpdatedAt:"Oct 16, 2025",frontMatter:{},sidebar:"docs",previous:{title:"Installation Runtime Proxy",permalink:"/docs/next/user-manuals/installation-runtime-proxy"},next:{title:"PodMigrationJob",permalink:"/docs/next/user-manuals/pod-migration-job"}},i={},d=[{value:"Introduction",id:"introduction",level:2},{value:"Setup",id:"setup",level:2},{value:"Prerequisite",id:"prerequisite",level:3},{value:"Installation",id:"installation",level:3},{value:"Configurations",id:"configurations",level:3},{value:"Use Resource Reservation",id:"use-resource-reservation",level:2},{value:"Quick Start",id:"quick-start",level:3},{value:"Advanced Configurations",id:"advanced-configurations",level:3},{value:"Field: <code>allocateOnce</code>",id:"field-allocateonce",level:4},{value:"Field: <code>allocatePolicy</code>",id:"field-allocatepolicy",level:4},{value:"Field: <code>preAllocation</code>",id:"field-preallocation",level:4},{value:"Field: <code>unschedulable</code>",id:"field-unschedulable",level:4},{value:"Field: <code>taints</code>",id:"field-taints",level:4},{value:"Example: Reserve on Specified Node, with Multiple Owners",id:"example-reserve-on-specified-node-with-multiple-owners",level:3}],c={toc:d},p="wrapper";function u(e){let{components:n,...r}=e;return(0,o.kt)(p,(0,a.Z)({},c,r,{components:n,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"resource-reservation"},"Resource Reservation"),(0,o.kt)("p",null,"Resource Reservation is an ability of koord-scheduler for reserving node resources for specific pods or workloads."),(0,o.kt)("h2",{id:"introduction"},"Introduction"),(0,o.kt)("p",null,"Pods are fundamental for allocating node resources in Kubernetes, which bind resource requirements with business logic.\nHowever, we may allocate resources for specific pods or workloads not created yet in the scenarios below:"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},'Preemption: Existing preemption does not guarantee that only preempting pods can allocate preempted resources. We expect that the scheduler can "lock" resources preventing from allocation of other pods even if they have the same or higher priorities.'),(0,o.kt)("li",{parentName:"ol"},"De-scheduling: For the descheduler, it is better to ensure sufficient resources before pods get rescheduled. Otherwise, rescheduled pods may not be runnable anymore and make the belonging application disrupted."),(0,o.kt)("li",{parentName:"ol"},"Horizontal scaling: To achieve more deterministic horizontal scaling, we expect to allocate node resources for the replicas to scale."),(0,o.kt)("li",{parentName:"ol"},"Resource Pre-allocation: We may want to pre-allocate node resources for future resource demands even if the resources are not currently allocatable.")),(0,o.kt)("p",null,"To enhance the resource scheduling of Kubernetes, koord-scheduler provides a scheduling API named ",(0,o.kt)("inlineCode",{parentName:"p"},"Reservation"),", which allows us to reserve node resources for specified pods or workloads even if they haven't get created yet."),(0,o.kt)("p",null,(0,o.kt)("img",{alt:"image",src:t(4462).Z,width:"511",height:"371"})),(0,o.kt)("p",null,"For more information, please see ",(0,o.kt)("a",{parentName:"p",href:"../designs/resource-reservation"},"Design: Resource Reservation"),"."),(0,o.kt)("h2",{id:"setup"},"Setup"),(0,o.kt)("h3",{id:"prerequisite"},"Prerequisite"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Kubernetes >= 1.18"),(0,o.kt)("li",{parentName:"ul"},"Koordinator >= 0.6")),(0,o.kt)("h3",{id:"installation"},"Installation"),(0,o.kt)("p",null,"Please make sure Koordinator components are correctly installed in your cluster. If not, please refer to ",(0,o.kt)("a",{parentName:"p",href:"/docs/installation"},"Installation"),"."),(0,o.kt)("h3",{id:"configurations"},"Configurations"),(0,o.kt)("p",null,"Resource Reservation is ",(0,o.kt)("em",{parentName:"p"},"Enabled")," by default. You can use it without any modification on the koord-scheduler config."),(0,o.kt)("h2",{id:"use-resource-reservation"},"Use Resource Reservation"),(0,o.kt)("h3",{id:"quick-start"},"Quick Start"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Deploy a reservation ",(0,o.kt)("inlineCode",{parentName:"li"},"reservation-demo")," with the YAML file below.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: scheduling.koordinator.sh/v1alpha1\nkind: Reservation\nmetadata:\n  name: reservation-demo\nspec:\n  template: # set resource requirements\n    metadata:\n      namespace: default\n    spec:\n      containers:\n        - args:\n            - '-c'\n            - '1'\n          command:\n            - stress\n          image: polinux/stress\n          imagePullPolicy: Always\n          name: stress\n          resources: # reserve 500m cpu and 800Mi memory\n            requests:\n              cpu: 500m\n              memory: 800Mi\n      schedulerName: koord-scheduler # use koord-scheduler\n  owners: # set the owner specifications\n    - object: # owner pods whose name is `default/pod-demo-0`\n        name: pod-demo-0\n        namespace: default\n  ttl: 1h # set the TTL, the reservation will get expired 1 hour later\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl create -f reservation-demo.yaml\nreservation.scheduling.koordinator.sh/reservation-demo created\n")),(0,o.kt)("ol",{start:2},(0,o.kt)("li",{parentName:"ol"},"Watch the reservation status util it becomes available.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get reservation reservation-demo -o wide\nNAME               PHASE       AGE   NODE     TTL  EXPIRES\nreservation-demo   Available   88s   node-0   1h\n")),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},"Deploy a pod ",(0,o.kt)("inlineCode",{parentName:"li"},"pod-demo-0")," with the YAML file below.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-demo-0 # match the owner spec of `reservation-demo`\nspec:\n  containers:\n    - args:\n        - '-c'\n        - '1'\n      command:\n        - stress\n      image: polinux/stress\n      imagePullPolicy: Always\n      name: stress\n      resources:\n        limits:\n          cpu: '1'\n          memory: 1Gi\n        requests:\n          cpu: 200m\n          memory: 400Mi\n  restartPolicy: Always\n  schedulerName: koord-scheduler # use koord-scheduler\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl create -f pod-demo-0.yaml\npod/pod-demo-0 created\n")),(0,o.kt)("ol",{start:4},(0,o.kt)("li",{parentName:"ol"},"Check the scheduled result of the pod ",(0,o.kt)("inlineCode",{parentName:"li"},"pod-demo-0"),".")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get pod pod-demo-0 -o wide\nNAME         READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES\npod-demo-0   1/1     Running   0          32s   10.17.0.123   node-0   <none>           <none>\n")),(0,o.kt)("p",null,(0,o.kt)("inlineCode",{parentName:"p"},"pod-demo-0")," is scheduled at the same node with ",(0,o.kt)("inlineCode",{parentName:"p"},"reservation-demo"),"."),(0,o.kt)("ol",{start:5},(0,o.kt)("li",{parentName:"ol"},"Check the status of the reservation ",(0,o.kt)("inlineCode",{parentName:"li"},"reservation-demo"),".")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'$ kubectl get reservation reservation-demo -oyaml\napiVersion: scheduling.koordinator.sh/v1alpha1\nkind: Reservation\nmetadata:\n  name: reservation-demo\n  creationTimestamp: "YYYY-MM-DDT05:24:58Z"\n  uid: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n  ...\nspec:\n  owners:\n  - object:\n      name: pod-demo-0\n      namespace: default\n  template:\n    spec:\n      containers:\n      - args:\n        - -c\n        - "1"\n        command:\n        - stress\n        image: polinux/stress\n        imagePullPolicy: Always\n        name: stress\n        resources:\n          requests:\n            cpu: 500m\n            memory: 800Mi\n      schedulerName: koord-scheduler\n  ttl: 1h\nstatus:\n  allocatable: # total reserved\n    cpu: 500m\n    memory: 800Mi\n  allocated: # current allocated\n    cpu: 200m\n    memory: 400Mi\n  conditions:\n  - lastProbeTime: "YYYY-MM-DDT05:24:58Z"\n    lastTransitionTime: "YYYY-MM-DDT05:24:58Z"\n    reason: Scheduled\n    status: "True"\n    type: Scheduled\n  - lastProbeTime: "YYYY-MM-DDT05:24:58Z"\n    lastTransitionTime: "YYYY-MM-DDT05:24:58Z"\n    reason: Available\n    status: "True"\n    type: Ready\n  currentOwners:\n  - name: pod-demo-0\n    namespace: default\n    uid: yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy\n  nodeName: node-0\n  phase: Available\n')),(0,o.kt)("p",null,"Now we can see the reservation ",(0,o.kt)("inlineCode",{parentName:"p"},"reservation-demo")," has reserved 500m cpu and 800Mi memory, and the pod ",(0,o.kt)("inlineCode",{parentName:"p"},"pod-demo-0"),"\nallocates 200m cpu and 400Mi memory from the reserved resources."),(0,o.kt)("ol",{start:6},(0,o.kt)("li",{parentName:"ol"},"Cleanup the reservation ",(0,o.kt)("inlineCode",{parentName:"li"},"reservation-demo"),".")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'$ kubectl delete reservation reservation-demo\nreservation.scheduling.koordinator.sh "reservation-demo" deleted\n$ kubectl get pod pod-demo-0\nNAME         READY   STATUS    RESTARTS   AGE\npod-demo-0   1/1     Running   0          110s\n')),(0,o.kt)("p",null,"After the reservation deleted, the pod ",(0,o.kt)("inlineCode",{parentName:"p"},"pod-demo-0")," is still running."),(0,o.kt)("h3",{id:"advanced-configurations"},"Advanced Configurations"),(0,o.kt)("blockquote",null,(0,o.kt)("p",{parentName:"blockquote"},"The latest API can be found in ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/koordinator-sh/koordinator/blob/main/apis/scheduling/v1alpha1/reservation_types.go"},(0,o.kt)("inlineCode",{parentName:"a"},"reservation_types")),".")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: scheduling.koordinator.sh/v1alpha1\nkind: Reservation\nmetadata:\n  name: reservation-demo\nspec:\n  # pod template (required): Reserve resources and play pod/node affinities according to the template.\n  # The resource requirements of the pod indicates the resource requirements of the reservation\n  template:\n    metadata:\n      namespace: default\n    spec:\n      containers:\n        - args:\n            - '-c'\n            - '1'\n          command:\n            - stress\n          image: polinux/stress\n          imagePullPolicy: Always\n          name: stress\n          resources:\n            requests:\n              cpu: 500m\n              memory: 800Mi\n      # scheduler name (required): use koord-scheduler to schedule the reservation\n      schedulerName: koord-scheduler\n  # owner spec (required): Specify what kinds of pods can allocate resources of this reservation.\n  # Currently support three kinds of owner specifications:\n  # - object: specify the name, namespace, uid of the owner pods\n  # - controller: specify the owner reference of the owner pods, e.g. name, namespace(extended by koordinator), uid, kind\n  # - labelSelector: specify the matching labels are matching expressions of the owner pods\n  owners:\n    - object:\n        name: pod-demo-0\n        namespace: default\n    - labelSelector:\n        matchLabels:\n          app: app-demo\n  # TTL (optional): Time-To-Live duration of the reservation. The reservation will get expired after the TTL period.\n  # If not set, use `24h` as default.\n  ttl: 1h\n  # Expires (optional): Expired timestamp when the reservation is expected to expire.\n  # If both `expires` and `ttl` are set, `expires` is checked first.\n  expires: \"YYYY-MM-DDTHH:MM:SSZ\"\n")),(0,o.kt)("h4",{id:"field-allocateonce"},"Field: ",(0,o.kt)("inlineCode",{parentName:"h4"},"allocateOnce")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Type: ",(0,o.kt)("inlineCode",{parentName:"li"},"*bool")),(0,o.kt)("li",{parentName:"ul"},"Default: ",(0,o.kt)("inlineCode",{parentName:"li"},"true")),(0,o.kt)("li",{parentName:"ul"},"Description: When set to ",(0,o.kt)("inlineCode",{parentName:"li"},"true"),", the reserved resources are only available for the first owner who allocates successfully and are not allocatable to other owners anymore. When set to ",(0,o.kt)("inlineCode",{parentName:"li"},"false"),", the reservation can be allocated by multiple owners as long as there are sufficient resources.")),(0,o.kt)("h4",{id:"field-allocatepolicy"},"Field: ",(0,o.kt)("inlineCode",{parentName:"h4"},"allocatePolicy")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Type: ",(0,o.kt)("inlineCode",{parentName:"li"},"ReservationAllocatePolicy")),(0,o.kt)("li",{parentName:"ul"},"Optional values: ",(0,o.kt)("inlineCode",{parentName:"li"},"Aligned"),", ",(0,o.kt)("inlineCode",{parentName:"li"},"Restricted")),(0,o.kt)("li",{parentName:"ul"},"Description: Specifies the allocation policy for the reservation.",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"Aligned"),": The Pod allocates resources from the Reservation first. If the remaining resources of the Reservation are insufficient, it can be allocated from the node, but it is required to strictly follow the resource specifications of the Pod. This avoids the problem that a Pod uses multiple Reservations at the same time."),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("inlineCode",{parentName:"li"},"Restricted"),": The resources requested by the Pod that overlap with the resources reserved by the Reservation can only be allocated from the Reservation. Resources declared in Pods but not reserved in Reservations can be allocated from Nodes. ",(0,o.kt)("inlineCode",{parentName:"li"},"Restricted")," includes the semantics of ",(0,o.kt)("inlineCode",{parentName:"li"},"Aligned"),".")))),(0,o.kt)("h4",{id:"field-preallocation"},"Field: ",(0,o.kt)("inlineCode",{parentName:"h4"},"preAllocation")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Type: ",(0,o.kt)("inlineCode",{parentName:"li"},"bool")),(0,o.kt)("li",{parentName:"ul"},"Default: ",(0,o.kt)("inlineCode",{parentName:"li"},"false")),(0,o.kt)("li",{parentName:"ul"},"Description: When ",(0,o.kt)("inlineCode",{parentName:"li"},"preAllocation")," is set to ",(0,o.kt)("inlineCode",{parentName:"li"},"true"),", the reservation can bind to already scheduled pods on nodes. The reservation will pre-allocate the resources from these running pods. When the bound pod terminates, the reservation automatically transitions from binding state to a normal reservation that reserves the freed resources.")),(0,o.kt)("p",null,"This is useful for scenarios like resource migration and graceful pod rescheduling, where you want to ensure resource continuity before a pod exits."),(0,o.kt)("h4",{id:"field-unschedulable"},"Field: ",(0,o.kt)("inlineCode",{parentName:"h4"},"unschedulable")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Type: ",(0,o.kt)("inlineCode",{parentName:"li"},"bool")),(0,o.kt)("li",{parentName:"ul"},"Default: ",(0,o.kt)("inlineCode",{parentName:"li"},"false")),(0,o.kt)("li",{parentName:"ul"},"Description: Controls reservation schedulability of new pods. By default, reservation is schedulable. When set to ",(0,o.kt)("inlineCode",{parentName:"li"},"true"),", no new pods can allocate this reservation.")),(0,o.kt)("h4",{id:"field-taints"},"Field: ",(0,o.kt)("inlineCode",{parentName:"h4"},"taints")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Type: ",(0,o.kt)("inlineCode",{parentName:"li"},"[]corev1.Taint")),(0,o.kt)("li",{parentName:"ul"},"Description: Specifies the reservation's taints. Pods must tolerate these taints to allocate the reserved resources.")),(0,o.kt)("h3",{id:"example-reserve-on-specified-node-with-multiple-owners"},"Example: Reserve on Specified Node, with Multiple Owners"),(0,o.kt)("ol",null,(0,o.kt)("li",{parentName:"ol"},"Check the resources allocatable of each node.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'$ kubectl get node -o custom-columns=NAME:.metadata.name,CPU:.status.allocatable.cpu,MEMORY:.status.allocatable.memory\nNAME     CPU     MEMORY\nnode-0   7800m   28625036Ki\nnode-1   7800m   28629692Ki\n...\n$ kubectl describe node node-1 | grep -A 8 "Allocated resources"\n  Allocated resources:\n    (Total limits may be over 100 percent, i.e., overcommitted.)\n    Resource                     Requests     Limits\n    --------                     --------     ------\n    cpu                          780m (10%)   7722m (99%)\n    memory                       1216Mi (4%)  14044Mi (50%)\n    ephemeral-storage            0 (0%)       0 (0%)\n    hugepages-1Gi                0 (0%)       0 (0%)\n    hugepages-2Mi                0 (0%)       0 (0%)\n')),(0,o.kt)("p",null,"As above, the node ",(0,o.kt)("inlineCode",{parentName:"p"},"node-1")," has about 7.0 cpu and 26Gi memory unallocated."),(0,o.kt)("ol",{start:2},(0,o.kt)("li",{parentName:"ol"},"Deploy a reservation ",(0,o.kt)("inlineCode",{parentName:"li"},"reservation-demo-big")," with the YAML file below.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: scheduling.koordinator.sh/v1alpha1\nkind: Reservation\nmetadata:\n  name: reservation-demo-big\nspec:\n  allocateOnce: false # allow the reservation to be allocated by multiple owners\n  template:\n    metadata:\n      namespace: default\n    spec:\n      containers:\n        - args:\n            - '-c'\n            - '1'\n          command:\n            - stress\n          image: polinux/stress\n          imagePullPolicy: Always\n          name: stress\n          resources: # reserve 6 cpu and 20Gi memory\n            requests:\n              cpu: 6\n              memory: 20Gi\n      nodeName: node-1 # set the expected node name to schedule at\n      schedulerName: koord-scheduler\n  owners: # set multiple owners\n    - object: # owner pods whose name is `default/pod-demo-0`\n        name: pod-demo-1\n        namespace: default\n    - labelSelector: # owner pods who have label `app=app-demo` can allocate the reserved resources\n        matchLabels:\n          app: app-demo\n  ttl: 1h\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl create -f reservation-demo-big.yaml\nreservation.scheduling.koordinator.sh/reservation-demo-big created\n")),(0,o.kt)("ol",{start:3},(0,o.kt)("li",{parentName:"ol"},"Watch the reservation status util it becomes available.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl get reservation reservation-demo-big -o wide\nNAME                   PHASE       AGE   NODE     TTL  EXPIRES\nreservation-demo-big   Available   37s   node-1   1h\n")),(0,o.kt)("p",null,"The reservation ",(0,o.kt)("inlineCode",{parentName:"p"},"reservation-demo-big")," is scheduled at the node ",(0,o.kt)("inlineCode",{parentName:"p"},"node-1"),", which matches the nodeName set in pod template."),(0,o.kt)("ol",{start:4},(0,o.kt)("li",{parentName:"ol"},"Deploy a deployment ",(0,o.kt)("inlineCode",{parentName:"li"},"app-demo")," with the YAML file below.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-yaml"},"apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: app-demo\n  template:\n    metadata:\n      name: stress\n      labels:\n        app: app-demo # match the owner spec of `reservation-demo-big`\n    spec:\n      schedulerName: koord-scheduler # use koord-scheduler\n      containers:\n      - name: stress\n        image: polinux/stress\n        args:\n          - '-c'\n          - '1'\n        command:\n          - stress\n        resources:\n          requests:\n            cpu: 2\n            memory: 10Gi\n          limits:\n            cpu: 4\n            memory: 20Gi\n")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"$ kubectl create -f app-demo.yaml\ndeployment.apps/app-demo created\n")),(0,o.kt)("ol",{start:5},(0,o.kt)("li",{parentName:"ol"},"Check the scheduled result of the pods of deployment ",(0,o.kt)("inlineCode",{parentName:"li"},"app-demo"),".")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},"k get pod -l app=app-demo -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP            NODE     NOMINATED NODE   READINESS GATES\napp-demo-798c66db46-ctnbr   1/1     Running   0          2m    10.17.0.124   node-1   <none>           <none>\napp-demo-798c66db46-pzphc   1/1     Running   0          2m    10.17.0.125   node-1   <none>           <none>\n")),(0,o.kt)("p",null,"Pods of deployment ",(0,o.kt)("inlineCode",{parentName:"p"},"app-demo")," are scheduled at the same node with ",(0,o.kt)("inlineCode",{parentName:"p"},"reservation-demo-big"),"."),(0,o.kt)("ol",{start:6},(0,o.kt)("li",{parentName:"ol"},"Check the status of the reservation ",(0,o.kt)("inlineCode",{parentName:"li"},"reservation-demo-big"),".")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-bash"},'$ kubectl get reservation reservation-demo-big -oyaml\napiVersion: scheduling.koordinator.sh/v1alpha1\nkind: Reservation\nmetadata:\n  name: reservation-demo-big\n  creationTimestamp: "YYYY-MM-DDT06:28:16Z"\n  uid: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n  ...\nspec:\n  allocateOnce: false # allow the reservation to be allocated by multiple owners\n  owners:\n  - object:\n      name: pod-demo-0\n      namespace: default\n  template:\n    spec:\n      containers:\n      - args:\n        - -c\n        - "1"\n        command:\n        - stress\n        image: polinux/stress\n        imagePullPolicy: Always\n        name: stress\n        resources:\n          requests:\n            cpu: 500m\n            memory: 800Mi\n      schedulerName: koord-scheduler\n  ttl: 1h\nstatus:\n  allocatable:\n    cpu: 6\n    memory: 20Gi\n  allocated:\n    cpu: 4\n    memory: 20Gi\n  conditions:\n  - lastProbeTime: "YYYY-MM-DDT06:28:17Z"\n    lastTransitionTime: "YYYY-MM-DDT06:28:17Z"\n    reason: Scheduled\n    status: "True"\n    type: Scheduled\n  - lastProbeTime: "YYYY-MM-DDT06:28:17Z"\n    lastTransitionTime: "YYYY-MM-DDT06:28:17Z"\n    reason: Available\n    status: "True"\n    type: Ready\n  currentOwners:\n  - name: app-demo-798c66db46-ctnbr\n    namespace: default\n    uid: yyyyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy\n  - name: app-demo-798c66db46-pzphc\n    namespace: default\n    uid: zzzzzzzz-zzzz-zzzz-zzzzzzzzzzzz\n  nodeName: node-1\n  phase: Available\n')),(0,o.kt)("p",null,"Now we can see the reservation ",(0,o.kt)("inlineCode",{parentName:"p"},"reservation-demo-big")," has reserved 6 cpu and 20Gi memory, and the pods of deployment\n",(0,o.kt)("inlineCode",{parentName:"p"},"app-demo")," allocates 4 cpu and 20Gi memory from the reserved resources.\nThe allocation for reserved resources does not increase the requested of node resources, otherwise the total request of\n",(0,o.kt)("inlineCode",{parentName:"p"},"node-1")," would exceed the node allocatable.\nMoreover, a reservation can be allocated by multiple owners when there are enough reserved resources unallocated."))}u.isMDXComponent=!0},4462:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/resource-reservation-0c5a187530dd5e3dc9c6e96f97add1ba.svg"}}]);